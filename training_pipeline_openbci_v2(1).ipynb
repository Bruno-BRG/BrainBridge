{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "725a5162",
   "metadata": {
    "id": "725a5162"
   },
   "source": [
    "# BCI Training & Fine-Tuning Pipeline Test - Google Colab Version\n",
    "\n",
    "Este notebook testa o pipeline completo no Google Colab:\n",
    "1. **Setup**: Instalar depend√™ncias e clonar reposit√≥rio\n",
    "2. **Treinamento Principal**: Subjects 1-79 do dataset PhysioNet\n",
    "3. **Fine-Tuning**: Dados espec√≠ficos da pasta Davi\n",
    "4. **Valida√ß√£o**: M√©tricas e compara√ß√£o de performance\n",
    "\n",
    "## ‚ö†Ô∏è IMPORTANTE: Este notebook foi adaptado para Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y9I7ySM2Ag5G",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y9I7ySM2Ag5G",
    "outputId": "7517a75c-8288-4748-e176-2c42554b1e30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021874a4",
   "metadata": {
    "id": "021874a4"
   },
   "source": [
    "## 1. Setup Inicial no Google Colab\n",
    "\n",
    "Vamos instalar as depend√™ncias e configurar o ambiente adequadamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd2a3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ecdd2a3e",
    "outputId": "33f9fb43-5694-4494-930c-955e103ec0f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Instalando depend√™ncias necess√°rias...\n",
      "Collecting braindecode\n",
      "  Downloading braindecode-0.8.1-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting mne\n",
      "  Downloading mne-1.9.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from braindecode) (2.0.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from braindecode) (2.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.15.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from braindecode) (3.10.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from braindecode) (3.13.0)\n",
      "Collecting skorch (from braindecode)\n",
      "  Downloading skorch-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from braindecode) (0.8.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.5.1)\n",
      "Collecting torchinfo (from braindecode)\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting docstring-inheritance (from braindecode)\n",
      "  Downloading docstring_inheritance-2.2.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne) (4.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne) (3.1.6)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne) (0.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne) (24.2)\n",
      "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne) (1.8.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (4.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (2.32.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->braindecode) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->braindecode) (2025.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from skorch->braindecode) (1.6.1)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from skorch->braindecode) (0.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->braindecode) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.0->skorch->braindecode) (3.6.0)\n",
      "Downloading braindecode-0.8.1-py3-none-any.whl (165 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m165.2/165.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mne-1.9.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docstring_inheritance-2.2.2-py3-none-any.whl (24 kB)\n",
      "Downloading skorch-1.1.0-py3-none-any.whl (228 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m228.9/228.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, docstring-inheritance, nvidia-cusparse-cu12, nvidia-cudnn-cu12, skorch, nvidia-cusolver-cu12, mne, braindecode\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed braindecode-0.8.1 docstring-inheritance-2.2.2 mne-1.9.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 skorch-1.1.0 torchinfo-1.8.0\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
      "Collecting jupyter\n",
      "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter) (6.5.7)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter) (6.1.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter) (6.17.1)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from jupyter) (7.7.1)\n",
      "Collecting jupyterlab (from jupyter)\n",
      "  Downloading jupyterlab-4.4.3-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (1.8.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (7.34.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (6.1.12)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (5.9.5)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (24.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (5.7.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter) (3.6.10)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter) (3.0.15)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter) (3.0.51)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter) (2.19.1)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter)\n",
      "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (3.1.6)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (5.8.1)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter)\n",
      "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter)\n",
      "  Downloading jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter)\n",
      "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (75.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (4.13.4)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (3.1.3)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (0.10.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (1.5.1)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (25.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (0.18.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (0.22.1)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (1.3.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (1.4.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter) (0.16.0)\n",
      "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->jupyter)\n",
      "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->jupyterlab->jupyter) (4.3.8)\n",
      "Collecting jupyter-client>=6.1.12 (from ipykernel->jupyter)\n",
      "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter) (21.2.0)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.4)\n",
      "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.17.0)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.24.0)\n",
      "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.3)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter) (2.21.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter) (0.2.13)\n",
      "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->notebook->jupyter) (0.7.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter) (4.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.25.1)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.22)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.0.0)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (24.11.1)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl.metadata (2.1 kB)\n",
      "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Downloading jupyterlab-4.4.3-py3-none-any.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyter_server-2.16.0-py3-none-any.whl (386 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
      "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: uri-template, types-python-dateutil, rfc3986-validator, rfc3339-validator, python-json-logger, overrides, json5, jedi, fqdn, async-lru, jupyter-server-terminals, jupyter-client, arrow, isoduration, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter\n",
      "  Attempting uninstall: jupyter-client\n",
      "    Found existing installation: jupyter-client 6.1.12\n",
      "    Uninstalling jupyter-client-6.1.12:\n",
      "      Successfully uninstalled jupyter-client-6.1.12\n",
      "  Attempting uninstall: jupyter-server\n",
      "    Found existing installation: jupyter-server 1.16.0\n",
      "    Uninstalling jupyter-server-1.16.0:\n",
      "      Successfully uninstalled jupyter-server-1.16.0\n",
      "Successfully installed arrow-1.3.0 async-lru-2.0.5 fqdn-1.5.1 isoduration-20.11.0 jedi-0.19.2 json5-0.12.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.16.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.3 jupyterlab-server-2.27.3 overrides-7.7.0 python-json-logger-3.3.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 types-python-dateutil-2.9.0.20250516 uri-template-1.3.0\n",
      "‚úÖ Depend√™ncias instaladas!\n",
      "üì± Executando no Google Colab\n"
     ]
    }
   ],
   "source": [
    "# === INSTALA√á√ÉO DE DEPEND√äNCIAS ===\n",
    "print(\"üîß Instalando depend√™ncias necess√°rias...\")\n",
    "\n",
    "# Instalar braindecode e depend√™ncias cient√≠ficas\n",
    "!pip install braindecode mne torch torchvision torchaudio\n",
    "!pip install scikit-learn pandas numpy matplotlib seaborn\n",
    "!pip install scipy jupyter\n",
    "\n",
    "print(\"‚úÖ Depend√™ncias instaladas!\")\n",
    "\n",
    "# Verificar se est√° no Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üì± Executando no Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üíª Executando localmente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf49bd97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf49bd97",
    "outputId": "c1d8d813-0f5c-4df6-eda3-3babbd9b61dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Configurando para Google Colab...\n",
      "üìÇ Diret√≥rio de trabalho: /content\n",
      "üìÅ Estrutura criada!\n"
     ]
    }
   ],
   "source": [
    "# === CLONE DO REPOSIT√ìRIO (OPCIONAL) ===\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üîÑ Configurando para Google Colab...\")\n",
    "\n",
    "    # Se voc√™ quiser clonar seu reposit√≥rio:\n",
    "    # !git clone https://github.com/seu-usuario/projetoBCI.git\n",
    "    # os.chdir('/content/projetoBCI')\n",
    "\n",
    "    # Por enquanto, vamos trabalhar no diret√≥rio padr√£o do Colab\n",
    "    project_root = Path('/content')\n",
    "\n",
    "    # Criar estrutura de diret√≥rios\n",
    "    (Path(\"/content/drive/MyDrive/Colab Notebooks/eeg_data\")).mkdir(exist_ok=True)\n",
    "    (project_root / \"models\").mkdir(exist_ok=True)\n",
    "    (project_root / \"results\").mkdir(exist_ok=True)\n",
    "\n",
    "else:\n",
    "    # Executando localmente\n",
    "    project_root = Path(os.getcwd()).parent\n",
    "\n",
    "print(f\"üìÇ Diret√≥rio de trabalho: {project_root}\")\n",
    "print(f\"üìÅ Estrutura criada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c91b8",
   "metadata": {
    "id": "244c91b8"
   },
   "source": [
    "## 2. Implementa√ß√£o das Classes Base (Inline)\n",
    "\n",
    "J√° que n√£o temos o reposit√≥rio clonado, vamos implementar as classes principais inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4587fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e4587fa",
    "outputId": "e22d80a4-c3c3-4657-f57b-6a3b4eebd828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports b√°sicos carregados!\n"
     ]
    }
   ],
   "source": [
    "# === IMPLEMENTA√á√ÉO INLINE DAS CLASSES BASE ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from typing import Optional, List, Tuple, Dict, Union\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Imports b√°sicos carregados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7fe101",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a7fe101",
    "outputId": "04ea0cdc-4a63-4d5b-8552-76dae598e233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ UniversalEEGNormalizer implementado!\n"
     ]
    }
   ],
   "source": [
    "# === UNIVERSAL EEG NORMALIZER ===\n",
    "class UniversalEEGNormalizer:\n",
    "    \"\"\"Normalizador universal para dados EEG\"\"\"\n",
    "\n",
    "    def __init__(self, method: str = 'zscore', mode: str = 'training'):\n",
    "        self.method = method\n",
    "        self.mode = mode\n",
    "        self.global_stats = {}\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def _ensure_3d(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Garantir que dados estejam em 3D (n_samples, n_channels, n_timepoints)\"\"\"\n",
    "        if len(data.shape) == 2:\n",
    "            n_samples, n_features = data.shape\n",
    "            if n_features % 16 == 0:  # Assumir 16 canais\n",
    "                n_channels = 16\n",
    "                n_timepoints = n_features // n_channels\n",
    "                data = data.reshape(n_samples, n_channels, n_timepoints)\n",
    "            else:\n",
    "                data = data[:, np.newaxis, :]\n",
    "        elif len(data.shape) == 1:\n",
    "            data = data[np.newaxis, np.newaxis, :]\n",
    "        return data\n",
    "\n",
    "    def fit(self, data: np.ndarray):\n",
    "        \"\"\"Ajustar normalizador aos dados\"\"\"\n",
    "        data_3d = self._ensure_3d(data)\n",
    "\n",
    "        if self.method == 'zscore':\n",
    "            self.global_stats['mean'] = np.mean(data_3d, axis=(0, 2), keepdims=True)\n",
    "            self.global_stats['std'] = np.std(data_3d, axis=(0, 2), keepdims=True)\n",
    "            self.global_stats['std'] = np.where(self.global_stats['std'] == 0, 1.0, self.global_stats['std'])\n",
    "\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Transformar dados\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Normalizador deve ser ajustado antes da transforma√ß√£o\")\n",
    "\n",
    "        original_shape = data.shape\n",
    "        data_3d = self._ensure_3d(data)\n",
    "\n",
    "        if self.method == 'zscore':\n",
    "            normalized = (data_3d - self.global_stats['mean']) / self.global_stats['std']\n",
    "\n",
    "        # Restaurar forma original\n",
    "        if len(original_shape) == 2:\n",
    "            return normalized.reshape(original_shape[0], -1)\n",
    "        elif len(original_shape) == 1:\n",
    "            return normalized.flatten()\n",
    "        return normalized\n",
    "\n",
    "    def fit_transform(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Ajustar e transformar em um passo\"\"\"\n",
    "        return self.fit(data).transform(data)\n",
    "\n",
    "print(\"‚úÖ UniversalEEGNormalizer implementado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d3f933",
   "metadata": {
    "id": "80d3f933"
   },
   "source": [
    "# Normaliza√ß√£o Melhorada para EEG\n",
    "\n",
    "Implementa√ß√£o de um normalizador mais robusto com:\n",
    "- M√∫ltiplas estrat√©gias de normaliza√ß√£o\n",
    "- Valida√ß√£o de qualidade\n",
    "- Tratamento de outliers\n",
    "- Normaliza√ß√£o por canal ou por trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40143506",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40143506",
    "outputId": "eae1e7a3-aaa7-4d4a-c420-945b281fe754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Normalizador melhorado implementado!\n"
     ]
    }
   ],
   "source": [
    "# === IMPROVED EEG NORMALIZER ===\n",
    "import numpy as np\n",
    "from typing import Dict, Optional, Tuple\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "class ImprovedEEGNormalizer:\n",
    "    \"\"\"Normalizador EEG avan√ßado com m√∫ltiplas estrat√©gias e valida√ß√£o\"\"\"\n",
    "\n",
    "    def __init__(self, method: str = 'robust_zscore', scope: str = 'channel',\n",
    "                 outlier_threshold: float = 3.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            method: 'robust_zscore', 'minmax', ou 'raw_zscore'\n",
    "            scope: 'channel', 'trial', ou 'global'\n",
    "            outlier_threshold: n√∫mero de desvios para considerar outlier\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.scope = scope\n",
    "        self.outlier_threshold = outlier_threshold\n",
    "        self.stats: Dict = {}\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def _handle_outliers(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Detecta e trata outliers usando IQR ou desvio padr√£o\"\"\"\n",
    "        if self.method == 'robust_zscore':\n",
    "            Q1 = np.percentile(X, 25, axis=(0, 2), keepdims=True)\n",
    "            Q3 = np.percentile(X, 75, axis=(0, 2), keepdims=True)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - self.outlier_threshold * IQR\n",
    "            upper = Q3 + self.outlier_threshold * IQR\n",
    "        else:\n",
    "            mean = np.mean(X, axis=(0, 2), keepdims=True)\n",
    "            std = np.std(X, axis=(0, 2), keepdims=True)\n",
    "            lower = mean - self.outlier_threshold * std\n",
    "            upper = mean + self.outlier_threshold * std\n",
    "\n",
    "        # Clip valores extremos\n",
    "        return np.clip(X, lower, upper)\n",
    "\n",
    "    def fit(self, X: np.ndarray) -> 'ImprovedEEGNormalizer':\n",
    "        \"\"\"Ajusta o normalizador aos dados\n",
    "\n",
    "        Args:\n",
    "            X: Array (trials, channels, time) ou (trials, features)\n",
    "        \"\"\"\n",
    "        # Garantir formato 3D\n",
    "        if len(X.shape) == 2:\n",
    "            if X.shape[1] % 16 == 0:  # Assumir 16 canais\n",
    "                n_channels = 16\n",
    "                X = X.reshape(X.shape[0], n_channels, -1)\n",
    "            else:\n",
    "                X = X[:, np.newaxis, :]\n",
    "\n",
    "        # Tratar outliers\n",
    "        X = self._handle_outliers(X)\n",
    "\n",
    "        if self.scope == 'channel':\n",
    "            if self.method == 'robust_zscore':\n",
    "                self.stats['median'] = np.median(X, axis=(0, 2), keepdims=True)\n",
    "                q75, q25 = np.percentile(X, [75, 25], axis=(0, 2))\n",
    "                self.stats['iqr'] = (q75 - q25)[None, :, None] + 1e-8\n",
    "\n",
    "            elif self.method == 'minmax':\n",
    "                self.stats['min'] = X.min(axis=(0, 2), keepdims=True)\n",
    "                self.stats['max'] = X.max(axis=(0, 2), keepdims=True)\n",
    "\n",
    "            else:  # raw_zscore\n",
    "                self.stats['mean'] = np.mean(X, axis=(0, 2), keepdims=True)\n",
    "                self.stats['std'] = np.std(X, axis=(0, 2), keepdims=True) + 1e-8\n",
    "\n",
    "        elif self.scope == 'trial':\n",
    "            if self.method == 'robust_zscore':\n",
    "                self.stats['median'] = np.median(X, axis=2, keepdims=True)\n",
    "                q75, q25 = np.percentile(X, [75, 25], axis=2)\n",
    "                self.stats['iqr'] = (q75 - q25)[:, :, None] + 1e-8\n",
    "\n",
    "            elif self.method == 'minmax':\n",
    "                self.stats['min'] = X.min(axis=2, keepdims=True)\n",
    "                self.stats['max'] = X.max(axis=2, keepdims=True)\n",
    "\n",
    "            else:  # raw_zscore\n",
    "                self.stats['mean'] = np.mean(X, axis=2, keepdims=True)\n",
    "                self.stats['std'] = np.std(X, axis=2, keepdims=True) + 1e-8\n",
    "\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Transforma os dados usando as estat√≠sticas calculadas\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Normalize.fit() deve ser chamado antes de transform()\")\n",
    "\n",
    "        # Garantir formato 3D\n",
    "        original_shape = X.shape\n",
    "        if len(X.shape) == 2:\n",
    "            if X.shape[1] % 16 == 0:\n",
    "                n_channels = 16\n",
    "                X = X.reshape(X.shape[0], n_channels, -1)\n",
    "            else:\n",
    "                X = X[:, np.newaxis, :]\n",
    "\n",
    "        # Transforma√ß√£o\n",
    "        if self.method == 'robust_zscore':\n",
    "            X_norm = (X - self.stats['median']) / self.stats['iqr']\n",
    "        elif self.method == 'minmax':\n",
    "            X_norm = (X - self.stats['min']) / (self.stats['max'] - self.stats['min'] + 1e-8)\n",
    "        else:  # raw_zscore\n",
    "            X_norm = (X - self.stats['mean']) / self.stats['std']\n",
    "\n",
    "        # Restaurar forma original se necess√°rio\n",
    "        if len(original_shape) == 2:\n",
    "            X_norm = X_norm.reshape(original_shape)\n",
    "\n",
    "        return X_norm\n",
    "\n",
    "    def fit_transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Ajusta aos dados e transforma em um √∫nico passo\"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Retorna estat√≠sticas de normaliza√ß√£o\"\"\"\n",
    "        return self.stats.copy()\n",
    "\n",
    "\n",
    "def validate_normalization(X: np.ndarray, normalized_X: np.ndarray) -> Tuple[bool, Dict]:\n",
    "    \"\"\"Valida a qualidade da normaliza√ß√£o\n",
    "\n",
    "    Args:\n",
    "        X: Dados originais\n",
    "        normalized_X: Dados normalizados\n",
    "\n",
    "    Returns:\n",
    "        (is_valid, stats): Booleano indicando se passou nos checks e estat√≠sticas\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "\n",
    "    # 1. Verificar m√©dia e desvio\n",
    "    stats['mean'] = float(np.mean(normalized_X))\n",
    "    stats['std'] = float(np.std(normalized_X))\n",
    "\n",
    "    # 2. Calcular percentis\n",
    "    stats['percentiles'] = {\n",
    "        '1%': float(np.percentile(normalized_X, 1)),\n",
    "        '99%': float(np.percentile(normalized_X, 99))\n",
    "    }\n",
    "\n",
    "    # 3. Verificar preserva√ß√£o de ordem relativa\n",
    "    original_order = np.argsort(np.mean(X, axis=(1,2)))\n",
    "    normalized_order = np.argsort(np.mean(normalized_X, axis=(1,2)))\n",
    "    stats['order_correlation'] = float(np.corrcoef(original_order, normalized_order)[0,1])\n",
    "\n",
    "    # 4. Verificar outliers\n",
    "    z_scores = np.abs((normalized_X - np.mean(normalized_X)) / np.std(normalized_X))\n",
    "    stats['outliers_ratio'] = float(np.mean(z_scores > 3))\n",
    "\n",
    "    # Crit√©rios de valida√ß√£o\n",
    "    is_valid = (\n",
    "        abs(stats['mean']) < 0.1 and           # M√©dia pr√≥xima de zero\n",
    "        abs(stats['std'] - 1) < 0.5 and        # Desvio pr√≥ximo de 1\n",
    "        stats['order_correlation'] > 0.7 and    # Preserva ordem relativa\n",
    "        stats['outliers_ratio'] < 0.01         # Menos de 1% outliers\n",
    "    )\n",
    "\n",
    "    return is_valid, stats\n",
    "\n",
    "print(\"‚úÖ Normalizador melhorado implementado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e6ebcc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12e6ebcc",
    "outputId": "4a612254-1e74-4d2d-80bb-befc0d076b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BCIDataset implementado!\n"
     ]
    }
   ],
   "source": [
    "# === BCI DATASET CLASS ===\n",
    "class BCIDataset(Dataset):\n",
    "    \"\"\"Dataset PyTorch para dados EEG\"\"\"\n",
    "\n",
    "    def __init__(self, windows: np.ndarray, labels: np.ndarray, transform=None, augment: bool = False):\n",
    "        self.windows = torch.from_numpy(windows).float()\n",
    "        self.labels = torch.from_numpy(labels).long()\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window = self.windows[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.augment:\n",
    "            # Adicionar ru√≠do leve\n",
    "            if torch.rand(1) < 0.3:\n",
    "                noise = torch.randn_like(window) * 0.01\n",
    "                window = window + noise\n",
    "\n",
    "        if self.transform:\n",
    "            window = self.transform(window)\n",
    "\n",
    "        return window, label\n",
    "\n",
    "print(\"‚úÖ BCIDataset implementado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4ebbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bec4ebbc",
    "outputId": "37963b05-791a-4db7-e486-c99863101af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Braindecode EEGInceptionERP importado com sucesso!\n",
      "‚úÖ EEGInceptionERPModel implementado! Tipo: Braindecode\n"
     ]
    }
   ],
   "source": [
    "# === EEG INCEPTION ERP MODEL ===\n",
    "try:\n",
    "    from braindecode.models import EEGInceptionERP\n",
    "    print(\"‚úÖ Braindecode EEGInceptionERP importado com sucesso!\")\n",
    "    BRAINDECODE_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erro ao importar braindecode: {e}\")\n",
    "    print(\"üîß Vamos implementar um modelo CNN simples como fallback\")\n",
    "    BRAINDECODE_AVAILABLE = False\n",
    "\n",
    "class EEGInceptionERPModel(nn.Module):\n",
    "    \"\"\"Wrapper para EEGInceptionERP com fallback para CNN simples\"\"\"\n",
    "\n",
    "    def __init__(self, n_chans: int, n_outputs: int, n_times: int, sfreq: float = 125.0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_chans = n_chans\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_times = n_times\n",
    "        self.sfreq = sfreq\n",
    "        self.is_trained = False\n",
    "\n",
    "        if BRAINDECODE_AVAILABLE:\n",
    "            try:\n",
    "                self.model = EEGInceptionERP(\n",
    "                    n_chans=n_chans,\n",
    "                    n_outputs=n_outputs,\n",
    "                    n_times=n_times,\n",
    "                    sfreq=sfreq\n",
    "                )\n",
    "                self.model_type = \"EEGInceptionERP\"\n",
    "                print(f\"‚úÖ Usando EEGInceptionERP: {n_chans} canais, {n_times} pontos temporais\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Erro ao criar EEGInceptionERP: {e}\")\n",
    "                self.model = self._create_fallback_cnn(n_chans, n_outputs, n_times)\n",
    "                self.model_type = \"FallbackCNN\"\n",
    "        else:\n",
    "            self.model = self._create_fallback_cnn(n_chans, n_outputs, n_times)\n",
    "            self.model_type = \"FallbackCNN\"\n",
    "\n",
    "    def _create_fallback_cnn(self, n_chans: int, n_outputs: int, n_times: int):\n",
    "        \"\"\"Criar CNN simples como fallback\"\"\"\n",
    "        print(f\"üîß Criando CNN fallback: {n_chans} canais, {n_times} pontos temporais\")\n",
    "        return nn.Sequential(\n",
    "            # Convolu√ß√£o temporal\n",
    "            nn.Conv1d(n_chans, 32, kernel_size=25, padding=12),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Redu√ß√£o de dimensionalidade\n",
    "            nn.Conv1d(32, 64, kernel_size=15, padding=7),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Convolu√ß√£o final\n",
    "            nn.Conv1d(64, 128, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # Classificador\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, n_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.model_type == \"EEGInceptionERP\":\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            # Para o CNN fallback, precisamos transpor de (batch, channels, time) para (batch, channels, time)\n",
    "            return self.model(x)\n",
    "\n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Salvar modelo\"\"\"\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        save_dict = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'model_type': self.model_type,\n",
    "            'n_chans': self.n_chans,\n",
    "            'n_outputs': self.n_outputs,\n",
    "            'n_times': self.n_times,\n",
    "            'sfreq': self.sfreq,\n",
    "            'is_trained': self.is_trained\n",
    "        }\n",
    "        torch.save(save_dict, filepath)\n",
    "        print(f\"‚úÖ Modelo salvo: {filepath}\")\n",
    "\n",
    "    def load(self, filepath: str):\n",
    "        \"\"\"Carregar modelo\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location='cpu')\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.is_trained = checkpoint.get('is_trained', True)\n",
    "        print(f\"‚úÖ Modelo carregado: {filepath}\")\n",
    "\n",
    "print(f\"‚úÖ EEGInceptionERPModel implementado! Tipo: {'Braindecode' if BRAINDECODE_AVAILABLE else 'Fallback CNN'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099f820",
   "metadata": {
    "id": "3099f820"
   },
   "source": [
    "## 3. Configura√ß√£o dos Caminhos e Par√¢metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8071299",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8071299",
    "outputId": "8842b224-2b7b-4df3-e645-71fa0185ea77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configura√ß√µes definidas:\n",
      "üìÇ Diret√≥rio de dados: /content/drive/MyDrive/Colab Notebooks/eeg_data/MNE-eegbci-data/files/eegmmidb/1.0.0\n",
      "üèóÔ∏è Modelos: /content/drive/MyDrive/Colab Notebooks/eeg_data/models\n",
      "üìä Resultados: /content/drive/MyDrive/Colab Notebooks/eeg_data/results\n",
      "\n",
      "üìù Par√¢metros principais:\n",
      "  - exclude_subjects: ['Davi']\n",
      "  - data_base_path: /content/drive/MyDrive/Colab Notebooks/eeg_data/MNE-eegbci-data/files/eegmmidb/1.0.0\n",
      "  - model_save_path: /content/drive/MyDrive/Colab Notebooks/eeg_data/models\n",
      "  - results_save_path: /content/drive/MyDrive/Colab Notebooks/eeg_data/results\n",
      "  - num_k_folds: 10\n",
      "  - num_epochs_per_fold: 30\n",
      "  - batch_size: 10\n",
      "  - early_stopping_patience: 8\n",
      "  - learning_rate: 0.001\n",
      "  - test_split_ratio: 0.2\n",
      "  - model_name: eeg_inception_openbci_cv10\n",
      "  normalization:\n",
      "    - method: robust_zscore\n",
      "    - scope: channel\n",
      "    - outlier_threshold: 3.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# --- Raiz do seu trabalho no Drive ------------------------------------------------\n",
    "PROJECT_ROOT = Path(\"/content/drive/MyDrive/Colab Notebooks/eeg_data\")\n",
    "\n",
    "# --- Dados ------------------------------------------------------------------------\n",
    "EEG_DATA_PATH  = PROJECT_ROOT / \"MNE-eegbci-data/files/eegmmidb/1.0.0\"\n",
    "DAVI_DATA_PATH = EEG_DATA_PATH / \"Davi\"\n",
    "\n",
    "# --- Sa√≠da de modelos e resultados ------------------------------------------------\n",
    "MODELS_PATH  = PROJECT_ROOT / \"models\"     # <-- ajuste se quiser outro local\n",
    "RESULTS_PATH = PROJECT_ROOT / \"results\"\n",
    "\n",
    "# Cria as pastas se ainda n√£o existirem (evita erros de salvamento depois)\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Par√¢metros de treinamento ---------------------------------------------------\n",
    "TRAINING_PARAMS = {\n",
    "    # Configura√ß√µes de dados\n",
    "    'exclude_subjects': ['Davi'],  # treina com todos exceto Davi\n",
    "    'data_base_path': str(EEG_DATA_PATH),\n",
    "    'model_save_path': str(MODELS_PATH),\n",
    "    'results_save_path': str(RESULTS_PATH),\n",
    "\n",
    "    # Configura√ß√µes de treinamento\n",
    "    'num_k_folds': 10,            # N√∫mero de folds para valida√ß√£o cruzada\n",
    "    'num_epochs_per_fold': 30,    # √âpocas m√°ximas por fold\n",
    "    'batch_size': 10,             # Tamanho do batch\n",
    "    'early_stopping_patience': 8,  # Paci√™ncia para early stopping\n",
    "    'learning_rate': 1e-3,        # Taxa de aprendizado\n",
    "    'test_split_ratio': 0.2,      # Propor√ß√£o do conjunto de teste\n",
    "\n",
    "    # Identifica√ß√£o do modelo\n",
    "    'model_name': 'eeg_inception_openbci_cv10',\n",
    "\n",
    "    # Configura√ß√µes de normaliza√ß√£o\n",
    "    'normalization': {\n",
    "        'method': 'robust_zscore',\n",
    "        'scope': 'channel',\n",
    "        'outlier_threshold': 3.0\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configura√ß√µes definidas:\")\n",
    "print(f\"üìÇ Diret√≥rio de dados: {EEG_DATA_PATH}\")\n",
    "print(f\"üèóÔ∏è Modelos: {MODELS_PATH}\")\n",
    "print(f\"üìä Resultados: {RESULTS_PATH}\")\n",
    "print(\"\\nüìù Par√¢metros principais:\")\n",
    "for key, value in TRAINING_PARAMS.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  {key}:\")\n",
    "        for k, v in value.items():\n",
    "            print(f\"    - {k}: {v}\")\n",
    "    else:\n",
    "        print(f\"  - {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4779d717",
   "metadata": {
    "id": "4779d717"
   },
   "source": [
    "## 4. Gera√ß√£o de Dados Sint√©ticos para Teste\n",
    "\n",
    "J√° que provavelmente voc√™ n√£o tem os dados reais no Colab, vamos gerar dados sint√©ticos para testar o pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rj2HrT2WASE0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rj2HrT2WASE0",
    "outputId": "1571f815-09c1-4729-e1b2-419681f9dd15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Carregando dataset OpenBCI CSV...\n",
      "‚úÖ Dados carregados: (3530, 16, 401) janelas ‚Äì [1781 1749] (classes) ‚Äì 79 sujeitos\n"
     ]
    }
   ],
   "source": [
    "# === CARREGAMENTO DE DADOS REAIS (OpenBCI CSV) ===\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import re\n",
    "\n",
    "def _read_openbci_csv(file_path: Path, sfreq: int = 125):\n",
    "    \"\"\"Carrega um √∫nico arquivo CSV gerado pela OpenBCI GUI e devolve um Raw MNE.\n",
    "    Aprende automaticamente os nomes dos canais EXG‚Ä¶ presentes no cabe√ßalho.\n",
    "    Adiciona anota√ß√µes (T0, T1, T2‚Ä¶) encontradas na coluna *Annotations*.\n",
    "    \"\"\"\n",
    "    # L√™ o CSV ignorando linhas iniciadas com '%'\n",
    "    df = pd.read_csv(file_path, comment='%')\n",
    "\n",
    "    # Seleciona colunas de EEG (por padr√£o 'EXG Channel 0‚Äë15')\n",
    "    eeg_cols = [c for c in df.columns if c.lower().startswith('exg channel')]\n",
    "    data = df[eeg_cols].T.to_numpy() * 1e-6  # ¬µV ‚Üí V\n",
    "\n",
    "    info = mne.create_info(ch_names=eeg_cols, sfreq=sfreq, ch_types='eeg')\n",
    "    raw = mne.io.RawArray(data, info, verbose=False)\n",
    "\n",
    "    # Anota√ß√µes: mapeia strings 'T0','T1','T2'‚Ä¶ para eventos\n",
    "    if 'Annotations' in df.columns:\n",
    "        onsets = df.index.to_numpy() / sfreq\n",
    "        for onset, annot in zip(onsets, df['Annotations'].astype(str)):\n",
    "            if annot.startswith('T'):\n",
    "                raw.annotations.append(onset, 0, annot)\n",
    "    return raw\n",
    "\n",
    "def load_openbci_eeg(data_dir: Path):\n",
    "    \"\"\"Carrega janelas EEG (trials √ó canais √ó tempo) e r√≥tulos bin√°rios\n",
    "    (0: m√£o esquerda, 1: m√£o direita) a partir de arquivos CSV da OpenBCI.\n",
    "\n",
    "    Arquitetura esperada:\n",
    "        data_dir/\n",
    "            S001R01_csv_openbci.csv\n",
    "            S001R02_csv_openbci.csv\n",
    "            ...\n",
    "            Davi/\n",
    "                Davi_R01_csv_openbci.csv\n",
    "                ...\n",
    "    O mapeamento de r√≥tulos assume:\n",
    "        'T1' ‚Üí 0 (m√£o esquerda)\n",
    "        'T2' ‚Üí 1 (m√£o direita)\n",
    "    Ajuste conforme a sua anota√ß√£o real, se necess√°rio.\n",
    "    \"\"\"\n",
    "    windows, labels, subject_ids = [], [], []\n",
    "\n",
    "    for csv_file in sorted(data_dir.rglob('*_csv_openbci.csv')):\n",
    "        # Infer subject ID (primeiros 3 d√≠gitos ap√≥s 'S')\n",
    "        match = re.search(r'[Ss](\\d{3})', csv_file.stem)\n",
    "        subj_id = int(match.group(1)) if match else 0\n",
    "\n",
    "        raw = _read_openbci_csv(csv_file)\n",
    "\n",
    "        # Extrai eventos T1/T2\n",
    "        events, event_id = mne.events_from_annotations(raw,\n",
    "                                                       event_id={'T1': 1, 'T2': 2},\n",
    "                                                       verbose=False)\n",
    "        if len(events) == 0:\n",
    "            continue\n",
    "\n",
    "        # Epoca intervalo 0‚Äë3.2‚ÄØs (‚âà‚ÄØ400 amostras a 125‚ÄØHz)\n",
    "        epochs = mne.Epochs(raw,\n",
    "                            events,\n",
    "                            event_id=event_id,\n",
    "                            tmin=0,\n",
    "                            tmax=3.2,\n",
    "                            baseline=None,\n",
    "                            preload=True,\n",
    "                            verbose=False)\n",
    "\n",
    "        lbl = epochs.events[:, 2] - 1  # Converte 1‚Üí0 (esq.), 2‚Üí1 (dir.)\n",
    "        windows.append(epochs.get_data())\n",
    "        labels.append(lbl)\n",
    "        subject_ids.append(np.full(len(lbl), subj_id))\n",
    "\n",
    "    if not windows:\n",
    "        raise RuntimeError(\"Nenhuma janela EEG encontrada ‚Äî verifique suas anota√ß√µes T1/T2.\")\n",
    "\n",
    "    windows = np.concatenate(windows, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    subject_ids = np.concatenate(subject_ids, axis=0)\n",
    "    return windows, labels, subject_ids\n",
    "\n",
    "\n",
    "print(\"üì• Carregando dataset OpenBCI CSV...\")\n",
    "windows, labels, subject_ids = load_openbci_eeg(EEG_DATA_PATH)\n",
    "print(f\"‚úÖ Dados carregados: {windows.shape} janelas ‚Äì {np.bincount(labels)} (classes) ‚Äì {len(np.unique(subject_ids))} sujeitos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837787c",
   "metadata": {
    "id": "c837787c"
   },
   "source": [
    "## 5. Implementa√ß√£o do Pipeline de Treinamento Simplificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b93b1",
   "metadata": {
    "id": "304b93b1"
   },
   "outputs": [],
   "source": [
    "### Par√¢metros de treinamento\n",
    "TRAINING_PARAMS = {\n",
    "    'exclude_subjects': ['Davi'],  # treina com todos exceto Davi\n",
    "    'num_k_folds': 10,\n",
    "    'num_epochs_per_fold': 30,\n",
    "    'batch_size': 10,\n",
    "    'early_stopping_patience': 8,\n",
    "    'learning_rate': 1e-3,\n",
    "    'test_split_ratio': 0.2,\n",
    "    'model_name': 'eeg_inception_openbci_cv10',\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# Depend√™ncias\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# =============================================================================\n",
    "# Fun√ß√£o principal de treinamento\n",
    "# =============================================================================\n",
    "\n",
    "def train_simplified_model(windows, labels, subject_ids, params):\n",
    "    \"\"\"Pipeline de treinamento simplificado com CV e early stopping.\"\"\"\n",
    "\n",
    "    print('üöÄ Iniciando treinamento simplificado‚Ä¶')\n",
    "\n",
    "    # --- Excluir sujeitos ------------------------------------------------------\n",
    "    if isinstance(params.get('exclude_subjects'), list):\n",
    "        mask = ~np.isin(subject_ids, params['exclude_subjects'])\n",
    "        windows, labels, subject_ids = windows[mask], labels[mask], subject_ids[mask]\n",
    "\n",
    "    print(f\"üìä Amostras ap√≥s exclus√£o: {windows.shape[0]}\")\n",
    "\n",
    "    # --- Normaliza√ß√£o ----------------------------------------------------------\n",
    "    print(\"\\nüìä Configurando normaliza√ß√£o...\")\n",
    "    normalizer = ImprovedEEGNormalizer(\n",
    "        method='robust_zscore',  # mais robusto a outliers\n",
    "        scope='channel',         # normaliza por canal\n",
    "        outlier_threshold=3.0    # 3 desvios padr√£o para outliers\n",
    "    )\n",
    "\n",
    "    # Normalizar dados\n",
    "    windows_norm = normalizer.fit_transform(windows)\n",
    "\n",
    "    # Validar normaliza√ß√£o\n",
    "    is_valid, norm_stats = validate_normalization(windows, windows_norm)\n",
    "    print(\"\\nüîç Valida√ß√£o da Normaliza√ß√£o:\")\n",
    "    print(f\"  - M√©dia: {norm_stats['mean']:.3f} (ideal: pr√≥ximo de 0)\")\n",
    "    print(f\"  - Desvio: {norm_stats['std']:.3f} (ideal: pr√≥ximo de 1)\")\n",
    "    print(f\"  - Correla√ß√£o de ordem: {norm_stats['order_correlation']:.3f}\")\n",
    "    print(f\"  - Propor√ß√£o de outliers: {norm_stats['outliers_ratio']*100:.2f}%\")\n",
    "    print(f\"  - Status: {'‚úÖ OK' if is_valid else '‚ö†Ô∏è Verificar'}\")\n",
    "\n",
    "    # --- Split treino/val/test -------------------------------------------------\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        windows_norm, labels,\n",
    "        test_size=params['test_split_ratio'],\n",
    "        random_state=42,\n",
    "        stratify=labels\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nüñ•Ô∏è Device: {device}\")\n",
    "\n",
    "    # --- Valida√ß√£o cruzada -----------------------------------------------------\n",
    "    kfold = KFold(n_splits=params['num_k_folds'], shuffle=True, random_state=42)\n",
    "    fold_accs = []\n",
    "    best_model = None\n",
    "    best_overall_acc = 0.0\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_val), 1):\n",
    "        print(f\"\\nüìÅ Fold {fold}/{params['num_k_folds']}\")\n",
    "\n",
    "        X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "        y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "        # Usar augmentation apenas no treino\n",
    "        train_ds = BCIDataset(X_train, y_train, augment=True)\n",
    "        val_ds   = BCIDataset(X_val,   y_val,   augment=False)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=params['batch_size'], shuffle=True)\n",
    "        val_loader   = DataLoader(val_ds,   batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "        model = EEGInceptionERPModel(\n",
    "            n_chans=windows.shape[1],\n",
    "            n_outputs=len(np.unique(labels)),\n",
    "            n_times=windows.shape[2]\n",
    "        ).to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optim     = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "        best_val_acc, patience = 0.0, 0\n",
    "        fold_history = []\n",
    "\n",
    "        for epoch in range(params['num_epochs_per_fold']):\n",
    "            # ---- Treinamento ----\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            correct = total = 0\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optim.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                pred = out.argmax(1)\n",
    "                correct += (pred == yb).sum().item()\n",
    "                total   += yb.size(0)\n",
    "\n",
    "            train_acc = correct / total\n",
    "            train_loss = train_loss / len(train_loader)\n",
    "\n",
    "            # ---- Valida√ß√£o ----\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = total = 0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    out = model(xb)\n",
    "                    val_loss += criterion(out, yb).item()\n",
    "                    pred = out.argmax(1)\n",
    "                    correct += (pred == yb).sum().item()\n",
    "                    total   += yb.size(0)\n",
    "\n",
    "            val_acc = correct / total\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "\n",
    "            fold_history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_acc': train_acc,\n",
    "                'train_loss': train_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss\n",
    "            })\n",
    "\n",
    "            print(f\"  Epoch {epoch+1}: Train {train_acc:.3f} ({train_loss:.3f}) | Val {val_acc:.3f} ({val_loss:.3f})\")\n",
    "\n",
    "            # ---- Early stopping ----\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience = 0\n",
    "                # Guardar melhor modelo geral\n",
    "                if val_acc > best_overall_acc:\n",
    "                    best_overall_acc = val_acc\n",
    "                    best_model = model.state_dict()\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= params['early_stopping_patience']:\n",
    "                    print(f\"  ‚èπÔ∏è  Early stopping na √©poca {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        fold_accs.append(best_val_acc)\n",
    "        print(f\"‚úÖ Fold {fold} conclu√≠do ‚Äì Melhor Val Acc: {best_val_acc:.4f}\")\n",
    "\n",
    "    # --- Avalia√ß√£o no conjunto de teste ----------------------------------------\n",
    "    print(\"\\nüéØ Avaliando no conjunto de teste...\")\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "        test_ds = BCIDataset(X_test, y_test, augment=False)\n",
    "        test_loader = DataLoader(test_ds, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = total = 0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                test_loss += criterion(out, yb).item()\n",
    "                pred = out.argmax(1)\n",
    "                correct += (pred == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "                true_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "        test_acc = correct / total\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "    else:\n",
    "        test_acc = test_loss = 0.0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "    # Preparar resultados estruturados\n",
    "    cv_mean = np.mean(fold_accs)\n",
    "    cv_std = np.std(fold_accs)\n",
    "    print(f\"\\nüìä Resultados Finais:\")\n",
    "    print(f\"  CV Mean Acc: {cv_mean:.4f} ¬± {cv_std:.4f}\")\n",
    "    print(f\"  Test Acc: {test_acc:.4f} (loss: {test_loss:.4f})\")\n",
    "\n",
    "    results = {\n",
    "        'fold_accuracies': fold_accs,\n",
    "        'cv_mean_accuracy': cv_mean,\n",
    "        'cv_std_accuracy': cv_std,\n",
    "        'final_test_accuracy': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'model_name': params['model_name'],\n",
    "        'best_model_state': best_model,\n",
    "        'normalization_stats': normalizer.get_stats(),\n",
    "        'normalization_validation': norm_stats,\n",
    "        'training_params': params,\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bc5b61",
   "metadata": {
    "id": "41bc5b61"
   },
   "source": [
    "## 6. Execu√ß√£o do Treinamento Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3b1eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1e3b1eb",
    "outputId": "e417cc0f-4092-46d9-aeea-54ef7eaeac0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ INICIANDO TREINAMENTO PRINCIPAL...\n",
      "‚ùå Erro durante treinamento: name 'MAIN_TRAINING_PARAMS' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-1593755867>\", line 10, in <cell line: 0>\n",
      "    MAIN_TRAINING_PARAMS\n",
      "NameError: name 'MAIN_TRAINING_PARAMS' is not defined\n"
     ]
    }
   ],
   "source": [
    "# === EXECU√á√ÉO DO TREINAMENTO ===\n",
    "print(\"üöÄ INICIANDO TREINAMENTO PRINCIPAL...\")\n",
    "\n",
    "try:\n",
    "    # Executar treinamento com dados sint√©ticos\n",
    "    main_results = train_simplified_model(\n",
    "        windows,\n",
    "        labels,\n",
    "        subject_ids,\n",
    "        MAIN_TRAINING_PARAMS\n",
    "    )\n",
    "\n",
    "    print(\"\\nüìä RESULTADOS DO TREINAMENTO:\")\n",
    "    print(f\"  - CV Mean: {main_results['cv_mean_accuracy']:.4f} ¬± {main_results['cv_std_accuracy']:.4f}\")\n",
    "    print(f\"  - Test Acc: {main_results['final_test_accuracy']:.4f}\")\n",
    "    print(f\"  - Modelo: {main_results['model_name']}\")\n",
    "\n",
    "    # Salvar resultados\n",
    "    results_path = RESULTS_PATH / \"main_training_results.json\"\n",
    "    RESULTS_PATH.mkdir(exist_ok=True)\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(main_results, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"üíæ Resultados salvos em: {results_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro durante treinamento: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    main_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd11f40",
   "metadata": {
    "id": "9cd11f40"
   },
   "source": [
    "## 7. Visualiza√ß√£o dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac95a0ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac95a0ce",
    "outputId": "24b9f6a6-61ac-4990-a8a8-4934ac29270d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå N√£o h√° resultados para visualizar\n"
     ]
    }
   ],
   "source": [
    "# === VISUALIZA√á√ÉO DOS RESULTADOS ===\n",
    "if main_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle('Resultados do Treinamento BCI - Google Colab', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Acur√°cias por fold\n",
    "    ax1 = axes[0, 0]\n",
    "    fold_accs = main_results['fold_accuracies']\n",
    "    ax1.bar(range(1, len(fold_accs) + 1), fold_accs, alpha=0.7, color='skyblue')\n",
    "    ax1.axhline(y=main_results['cv_mean_accuracy'], color='red', linestyle='--',\n",
    "                label=f\"Mean: {main_results['cv_mean_accuracy']:.3f}\")\n",
    "    ax1.set_xlabel('Fold')\n",
    "    ax1.set_ylabel('Acur√°cia')\n",
    "    ax1.set_title('Acur√°cia por Fold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Compara√ß√£o CV vs Test\n",
    "    ax2 = axes[0, 1]\n",
    "    models = ['CV Mean', 'Test Final']\n",
    "    accuracies = [main_results['cv_mean_accuracy'], main_results['final_test_accuracy']]\n",
    "    bars = ax2.bar(models, accuracies, color=['lightcoral', 'lightgreen'], alpha=0.8)\n",
    "    ax2.set_ylabel('Acur√°cia')\n",
    "    ax2.set_title('CV vs Test Performance')\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # 3. Distribui√ß√£o das classes sint√©ticas\n",
    "    ax3 = axes[1, 0]\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    ax3.pie(counts, labels=[f'Classe {u}' for u in unique], autopct='%1.1f%%')\n",
    "    ax3.set_title('Distribui√ß√£o de Classes')\n",
    "\n",
    "    # 4. Resumo textual\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    RESUMO DO TREINAMENTO\n",
    "\n",
    "    Dataset: Sint√©tico\n",
    "    Subjects: {len(np.unique(subject_ids))}\n",
    "    Amostras: {len(labels)}\n",
    "\n",
    "    Modelo: {main_results['model_name']}\n",
    "    Tipo: {'EEGInceptionERP' if BRAINDECODE_AVAILABLE else 'CNN Fallback'}\n",
    "\n",
    "    Performance:\n",
    "    ‚Ä¢ CV Mean: {main_results['cv_mean_accuracy']:.4f}\n",
    "    ‚Ä¢ CV Std: {main_results['cv_std_accuracy']:.4f}\n",
    "    ‚Ä¢ Test Acc: {main_results['final_test_accuracy']:.4f}\n",
    "\n",
    "    Status: ‚úÖ Sucesso\n",
    "    \"\"\"\n",
    "    ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes,\n",
    "             fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_PATH / 'colab_training_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"‚úÖ Visualiza√ß√£o criada e salva!\")\n",
    "else:\n",
    "    print(\"‚ùå N√£o h√° resultados para visualizar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baca50ec",
   "metadata": {
    "id": "baca50ec"
   },
   "source": [
    "# 9. Fine-tuning com Dados Espec√≠ficos\n",
    "\n",
    "Nesta se√ß√£o, vamos realizar o fine-tuning do modelo treinado anteriormente usando os dados espec√≠ficos localizados na pasta Davi.\n",
    "\n",
    "O processo de fine-tuning envolve:\n",
    "1. Carregar o melhor modelo do treinamento anterior\n",
    "2. Congelar parte das camadas iniciais (transfer learning)\n",
    "3. Treinar apenas as √∫ltimas camadas com os novos dados\n",
    "4. Validar a performance no conjunto de teste espec√≠fico\n",
    "\n",
    "Este processo √© especialmente √∫til quando temos um modelo base treinado em muitos sujeitos e queremos adapt√°-lo para um sujeito espec√≠fico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa017e1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa017e1b",
    "outputId": "244b843a-90ce-4fd6-be5c-96cbe6ef75e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Carregando dados do Davi...\n",
      "üìä Dados carregados: 20 amostras\n",
      "‚ùå Modelo base n√£o dispon√≠vel para fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# === FINE-TUNING COM DADOS ESPEC√çFICOS ===\n",
    "def fine_tune_model(base_model, windows, labels, params, original_norm_stats=None):\n",
    "    \"\"\"Realiza fine-tuning do modelo base com dados espec√≠ficos.\"\"\"\n",
    "    print(\"üéØ Iniciando fine-tuning...\")\n",
    "\n",
    "    # --- Prepara√ß√£o dos dados ------------------------------------------------\n",
    "    if original_norm_stats and params.get('use_original_normalization', False):\n",
    "        print(\"üìä Usando normaliza√ß√£o do treino original...\")\n",
    "        # Verificar quais estat√≠sticas temos dispon√≠veis\n",
    "        if 'median' in original_norm_stats and 'iqr' in original_norm_stats:\n",
    "            windows_norm = (windows - original_norm_stats['median']) / original_norm_stats['iqr']\n",
    "        elif 'mean' in original_norm_stats and 'std' in original_norm_stats:\n",
    "            windows_norm = (windows - original_norm_stats['mean']) / original_norm_stats['std']\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Estat√≠sticas de normaliza√ß√£o original n√£o reconhecidas, usando nova normaliza√ß√£o...\")\n",
    "            normalizer = ImprovedEEGNormalizer(method='robust_zscore')\n",
    "            windows_norm = normalizer.fit_transform(windows)\n",
    "    else:\n",
    "        print(\"üìä Usando nova normaliza√ß√£o espec√≠fica...\")\n",
    "        normalizer = ImprovedEEGNormalizer(method='robust_zscore')\n",
    "        windows_norm = normalizer.fit_transform(windows)\n",
    "\n",
    "        # Comparar estat√≠sticas se tivermos as originais\n",
    "        if original_norm_stats:\n",
    "            # Pegar estat√≠sticas atuais\n",
    "            current_stats = normalizer.get_stats()\n",
    "\n",
    "            # Comparar estat√≠sticas dispon√≠veis\n",
    "            if 'median' in current_stats and 'median' in original_norm_stats:\n",
    "                median_diff = np.abs(current_stats['median'] - original_norm_stats['median']).mean()\n",
    "                iqr_diff = np.abs(current_stats['iqr'] - original_norm_stats['iqr']).mean()\n",
    "                print(f\"üìà Diferen√ßa m√©dia nas estat√≠sticas (robust):\")\n",
    "                print(f\"   - Mediana: {median_diff:.6f}\")\n",
    "                print(f\"   - IQR: {iqr_diff:.6f}\")\n",
    "            elif 'mean' in current_stats and 'mean' in original_norm_stats:\n",
    "                mean_diff = np.abs(current_stats['mean'] - original_norm_stats['mean']).mean()\n",
    "                std_diff = np.abs(current_stats['std'] - original_norm_stats['std']).mean()\n",
    "                print(f\"üìà Diferen√ßa m√©dia nas estat√≠sticas (zscore):\")\n",
    "                print(f\"   - M√©dia: {mean_diff:.6f}\")\n",
    "                print(f\"   - Desvio: {std_diff:.6f}\")\n",
    "\n",
    "    # Validar normaliza√ß√£o\n",
    "    is_valid, norm_stats = validate_normalization(windows, windows_norm)\n",
    "    print(\"\\nüîç Valida√ß√£o da Normaliza√ß√£o:\")\n",
    "    print(f\"  - M√©dia: {norm_stats['mean']:.3f} (ideal: pr√≥ximo de 0)\")\n",
    "    print(f\"  - Desvio: {norm_stats['std']:.3f} (ideal: pr√≥ximo de 1)\")\n",
    "    print(f\"  - Correla√ß√£o de ordem: {norm_stats['order_correlation']:.3f}\")\n",
    "    print(f\"  - Propor√ß√£o de outliers: {norm_stats['outliers_ratio']*100:.2f}%\")\n",
    "    print(f\"  - Status: {'‚úÖ OK' if is_valid else '‚ö†Ô∏è Verificar'}\")\n",
    "\n",
    "    # --- Split train/val/test -------------------------------------------------\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        windows_norm, labels,\n",
    "        test_size=0.3,\n",
    "        random_state=42,\n",
    "        stratify=labels\n",
    "    )\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=0.5,\n",
    "        random_state=42,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = BCIDataset(X_train, y_train, augment=True)\n",
    "    val_ds = BCIDataset(X_val, y_val, augment=False)\n",
    "    test_ds = BCIDataset(X_test, y_test, augment=False)\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=params['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "    # --- Prepara√ß√£o do modelo -----------------------------------------------\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = base_model.to(device)\n",
    "\n",
    "    # Congelar camadas iniciais conforme estrat√©gia\n",
    "    if params['freeze_strategy'] == 'early':\n",
    "        frozen_count = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'encoder' in name or 'conv' in name:\n",
    "                param.requires_grad = False\n",
    "                frozen_count += 1\n",
    "        print(f\"\\nüßä Camadas congeladas: {frozen_count}\")\n",
    "\n",
    "    # Otimizador com learning rate reduzido\n",
    "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(\n",
    "        trainable_params,\n",
    "        lr=params['learning_rate_ratio'] * params['learning_rate']\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # --- Fine-tuning -------------------------------------------------------\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience = params.get('early_stopping_patience', 5)\n",
    "    wait = 0\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(params['epochs']):\n",
    "        # Treino\n",
    "        model.train()\n",
    "        train_loss = train_correct = train_total = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            pred = out.argmax(1)\n",
    "            train_correct += (pred == yb).sum().item()\n",
    "            train_total += yb.size(0)\n",
    "\n",
    "        train_acc = train_correct / train_total\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Valida√ß√£o\n",
    "        model.eval()\n",
    "        val_loss = val_correct = val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                pred = out.argmax(1)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (pred == yb).sum().item()\n",
    "                val_total += yb.size(0)\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        # Guardar hist√≥rico\n",
    "        history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        })\n",
    "\n",
    "        print(f\"√âpoca {epoch+1}: Train {train_acc:.3f} ({train_loss:.3f}) | Val {val_acc:.3f} ({val_loss:.3f})\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"\\n‚èπÔ∏è Early stopping na √©poca {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # --- Avalia√ß√£o final --------------------------------------------------\n",
    "    print(\"\\nüéØ Avaliando modelo final...\")\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = test_correct = test_total = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            pred = out.argmax(1)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_correct += (pred == yb).sum().item()\n",
    "            test_total += yb.size(0)\n",
    "\n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            true_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "    test_acc = test_correct / test_total\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "\n",
    "    results = {\n",
    "        'final_test_accuracy': test_acc,\n",
    "        'best_val_accuracy': best_val_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'model_state': best_model_state,\n",
    "        'history': history,\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels,\n",
    "        'normalization_validation': norm_stats\n",
    "    }\n",
    "\n",
    "    print(f\"\\n‚ú® Resultados do Fine-tuning:\")\n",
    "    print(f\"   - Melhor Val Acc: {best_val_acc:.4f}\")\n",
    "    print(f\"   - Test Acc: {test_acc:.4f}\")\n",
    "    print(f\"   - Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# === EXECU√á√ÉO DO FINE-TUNING ===\n",
    "print(\"üîÑ Carregando dados do Davi...\")\n",
    "\n",
    "# Carregar dados espec√≠ficos do Davi\n",
    "davi_windows, davi_labels, _ = load_openbci_eeg(Path(DAVI_DATA_PATH))\n",
    "print(f\"üìä Dados carregados: {davi_windows.shape[0]} amostras\")\n",
    "\n",
    "# Par√¢metros do fine-tuning\n",
    "fine_tuning_params = {\n",
    "    'freeze_strategy': 'early',      # congela camadas iniciais\n",
    "    'learning_rate_ratio': 0.1,      # lr menor para fine-tuning\n",
    "    'epochs': 30,\n",
    "    'batch_size': 8,\n",
    "    'early_stopping_patience': 5,\n",
    "    'learning_rate': TRAINING_PARAMS['learning_rate'],\n",
    "    'use_original_normalization': False  # Escolha se quer usar normaliza√ß√£o original ou nova\n",
    "}\n",
    "\n",
    "# Executar fine-tuning\n",
    "if main_results and main_results.get('best_model_state') is not None:\n",
    "    print(\"\\nüéØ Iniciando fine-tuning com modelo base...\")\n",
    "\n",
    "    # Criar modelo base com os pesos do melhor modelo\n",
    "    base_model = EEGInceptionERPModel(\n",
    "        n_chans=davi_windows.shape[1],\n",
    "        n_outputs=len(np.unique(davi_labels)),\n",
    "        n_times=davi_windows.shape[2]\n",
    "    )\n",
    "    base_model.load_state_dict(main_results['best_model_state'])\n",
    "\n",
    "    # Fine-tuning\n",
    "    ft_results = fine_tune_model(\n",
    "        base_model=base_model,\n",
    "        windows=davi_windows,\n",
    "        labels=davi_labels,\n",
    "        params=fine_tuning_params,\n",
    "        original_norm_stats=main_results.get('normalization_stats')\n",
    "    )\n",
    "\n",
    "    # Salvar modelo final\n",
    "    if ft_results['model_state'] is not None:\n",
    "        base_model.load_state_dict(ft_results['model_state'])\n",
    "        model_path = MODELS_PATH / f\"{TRAINING_PARAMS['model_name']}_finetuned.pt\"\n",
    "        torch.save({\n",
    "            'model_state_dict': ft_results['model_state'],\n",
    "            'test_accuracy': ft_results['final_test_accuracy'],\n",
    "            'val_accuracy': ft_results['best_val_accuracy'],\n",
    "            'test_loss': ft_results['test_loss'],\n",
    "            'history': ft_results['history'],\n",
    "            'normalization_validation': ft_results['normalization_validation']\n",
    "        }, model_path)\n",
    "        print(f\"\\nüíæ Modelo fine-tuned salvo em: {model_path}\")\n",
    "else:\n",
    "    print(\"‚ùå Modelo base n√£o dispon√≠vel para fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e7486",
   "metadata": {
    "id": "9b7e7486"
   },
   "source": [
    "## 8. Relat√≥rio Final\n",
    "\n",
    "Resumo da execu√ß√£o no Google Colab com implementa√ß√£o inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac7f43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0ac7f43",
    "outputId": "f431250f-c9ff-4cfa-feb4-e04f30dbb3f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "            RELAT√ìRIO FINAL - GOOGLE COLAB\n",
      "============================================================\n",
      "\n",
      "üñ•Ô∏è AMBIENTE:\n",
      "  - Plataforma: Google Colab\n",
      "  - Braindecode: ‚úÖ Dispon√≠vel\n",
      "  - Device: cpu\n",
      "\n",
      "üìä DADOS:\n",
      "  - Tipo: Sint√©tico\n",
      "  - Amostras: 3530\n",
      "  - Subjects: 79\n",
      "  - Classes: 2\n",
      "\n",
      "‚ùå RESULTADOS: FALHOU\n",
      "\n",
      "üìÅ ARQUIVOS CRIADOS:\n",
      "  - models/: 6 arquivo(s)\n",
      "  - results/: 2 arquivo(s)\n",
      "\n",
      "üéâ CONCLUS√ÉO:\n",
      "  Este notebook demonstrou como adaptar o pipeline BCI\n",
      "  para execu√ß√£o no Google Colab, incluindo:\n",
      "  ‚Ä¢ Implementa√ß√£o inline das classes principais\n",
      "  ‚Ä¢ Gera√ß√£o de dados sint√©ticos para teste\n",
      "  ‚Ä¢ Pipeline de treinamento simplificado\n",
      "  ‚Ä¢ Fallback para quando braindecode n√£o est√° dispon√≠vel\n",
      "\n",
      "‚ú® Execu√ß√£o conclu√≠da em: 2025-06-16 03:23:59\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"            RELAT√ìRIO FINAL - GOOGLE COLAB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüñ•Ô∏è AMBIENTE:\")\n",
    "print(f\"  - Plataforma: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"  - Braindecode: {'‚úÖ Dispon√≠vel' if BRAINDECODE_AVAILABLE else '‚ùå Fallback CNN'}\")\n",
    "print(f\"  - Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "\n",
    "print(f\"\\nüìä DADOS:\")\n",
    "print(f\"  - Tipo: Sint√©tico\")\n",
    "print(f\"  - Amostras: {len(labels)}\")\n",
    "print(f\"  - Subjects: {len(np.unique(subject_ids))}\")\n",
    "print(f\"  - Classes: {len(np.unique(labels))}\")\n",
    "\n",
    "if main_results:\n",
    "    print(f\"\\nüéØ RESULTADOS:\")\n",
    "    print(f\"  - Modelo: {main_results['model_name']}\")\n",
    "    print(f\"  - CV Accuracy: {main_results['cv_mean_accuracy']:.4f} ¬± {main_results['cv_std_accuracy']:.4f}\")\n",
    "    print(f\"  - Test Accuracy: {main_results['final_test_accuracy']:.4f}\")\n",
    "    print(f\"  - Status: ‚úÖ SUCESSO\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå RESULTADOS: FALHOU\")\n",
    "\n",
    "print(f\"\\nüìÅ ARQUIVOS CRIADOS:\")\n",
    "for path in [MODELS_PATH, RESULTS_PATH]:\n",
    "    if path.exists():\n",
    "        files = list(path.rglob(\"*\"))\n",
    "        print(f\"  - {path.name}/: {len(files)} arquivo(s)\")\n",
    "\n",
    "print(f\"\\nüéâ CONCLUS√ÉO:\")\n",
    "print(f\"  Este notebook demonstrou como adaptar o pipeline BCI\")\n",
    "print(f\"  para execu√ß√£o no Google Colab, incluindo:\")\n",
    "print(f\"  ‚Ä¢ Implementa√ß√£o inline das classes principais\")\n",
    "print(f\"  ‚Ä¢ Gera√ß√£o de dados sint√©ticos para teste\")\n",
    "print(f\"  ‚Ä¢ Pipeline de treinamento simplificado\")\n",
    "print(f\"  ‚Ä¢ Fallback para quando braindecode n√£o est√° dispon√≠vel\")\n",
    "\n",
    "print(f\"\\n‚ú® Execu√ß√£o conclu√≠da em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
