{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "725a5162",
   "metadata": {
    "id": "725a5162"
   },
   "source": [
    "# BCI Training & Fine-Tuning Pipeline Test - Google Colab Version\n",
    "\n",
    "Este notebook testa o pipeline completo no Google Colab:\n",
    "1. **Setup**: Instalar dependências e clonar repositório\n",
    "2. **Treinamento Principal**: Subjects 1-79 do dataset PhysioNet\n",
    "3. **Fine-Tuning**: Dados específicos da pasta Davi\n",
    "4. **Validação**: Métricas e comparação de performance\n",
    "\n",
    "## ⚠️ IMPORTANTE: Este notebook foi adaptado para Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y9I7ySM2Ag5G",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y9I7ySM2Ag5G",
    "outputId": "7517a75c-8288-4748-e176-2c42554b1e30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021874a4",
   "metadata": {
    "id": "021874a4"
   },
   "source": [
    "## 1. Setup Inicial no Google Colab\n",
    "\n",
    "Vamos instalar as dependências e configurar o ambiente adequadamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd2a3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ecdd2a3e",
    "outputId": "33f9fb43-5694-4494-930c-955e103ec0f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Instalando dependências necessárias...\n",
      "Collecting braindecode\n",
      "  Downloading braindecode-0.8.1-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting mne\n",
      "  Downloading mne-1.9.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from braindecode) (2.0.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from braindecode) (2.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.15.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from braindecode) (3.10.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from braindecode) (3.13.0)\n",
      "Collecting skorch (from braindecode)\n",
      "  Downloading skorch-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from braindecode) (0.8.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from braindecode) (1.5.1)\n",
      "Collecting torchinfo (from braindecode)\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting docstring-inheritance (from braindecode)\n",
      "  Downloading docstring_inheritance-2.2.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne) (4.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne) (3.1.6)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne) (0.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne) (24.2)\n",
      "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne) (1.8.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->braindecode) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (4.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (2.32.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->braindecode) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->braindecode) (2025.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from skorch->braindecode) (1.6.1)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from skorch->braindecode) (0.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->braindecode) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.0->skorch->braindecode) (3.6.0)\n",
      "Downloading braindecode-0.8.1-py3-none-any.whl (165 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.2/165.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mne-1.9.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docstring_inheritance-2.2.2-py3-none-any.whl (24 kB)\n",
      "Downloading skorch-1.1.0-py3-none-any.whl (228 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.9/228.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, docstring-inheritance, nvidia-cusparse-cu12, nvidia-cudnn-cu12, skorch, nvidia-cusolver-cu12, mne, braindecode\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed braindecode-0.8.1 docstring-inheritance-2.2.2 mne-1.9.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 skorch-1.1.0 torchinfo-1.8.0\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
      "Collecting jupyter\n",
      "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter) (6.5.7)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter) (6.1.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter) (6.17.1)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from jupyter) (7.7.1)\n",
      "Collecting jupyterlab (from jupyter)\n",
      "  Downloading jupyterlab-4.4.3-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (1.8.0)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (7.34.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (6.1.12)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (1.6.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (5.9.5)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (24.0.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->jupyter) (5.7.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter) (3.6.10)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->jupyter) (3.0.15)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter) (3.0.51)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from jupyter-console->jupyter) (2.19.1)\n",
      "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter)\n",
      "  Downloading async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (0.28.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (3.1.6)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (5.8.1)\n",
      "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter)\n",
      "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter)\n",
      "  Downloading jupyter_server-2.16.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter)\n",
      "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter) (75.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (4.13.4)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (3.1.3)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (0.10.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (5.10.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter) (1.5.1)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (25.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (0.18.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (0.22.1)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter) (1.3.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter) (1.4.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (4.9.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter) (0.16.0)\n",
      "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->jupyter)\n",
      "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.7.5)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->jupyter) (4.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core->jupyterlab->jupyter) (4.3.8)\n",
      "Collecting jupyter-client>=6.1.12 (from ipykernel->jupyter)\n",
      "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (1.8.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter) (21.2.0)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter) (0.4)\n",
      "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.17.0)\n",
      "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter)\n",
      "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (4.24.0)\n",
      "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.32.3)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7->nbconvert->jupyter) (2.21.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter) (0.2.13)\n",
      "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->notebook->jupyter) (0.7.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter) (4.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->jupyter) (0.8.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (0.25.1)\n",
      "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.11/dist-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (6.0.2)\n",
      "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel->jupyter) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter) (2.4.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.22)\n",
      "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (3.0.0)\n",
      "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter) (24.11.1)\n",
      "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter)\n",
      "  Downloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl.metadata (2.1 kB)\n",
      "Downloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
      "Downloading jupyterlab-4.4.3-py3-none-any.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
      "Downloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyter_server-2.16.0-py3-none-any.whl (386 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
      "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
      "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
      "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
      "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
      "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
      "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
      "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading types_python_dateutil-2.9.0.20250516-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: uri-template, types-python-dateutil, rfc3986-validator, rfc3339-validator, python-json-logger, overrides, json5, jedi, fqdn, async-lru, jupyter-server-terminals, jupyter-client, arrow, isoduration, jupyter-events, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter\n",
      "  Attempting uninstall: jupyter-client\n",
      "    Found existing installation: jupyter-client 6.1.12\n",
      "    Uninstalling jupyter-client-6.1.12:\n",
      "      Successfully uninstalled jupyter-client-6.1.12\n",
      "  Attempting uninstall: jupyter-server\n",
      "    Found existing installation: jupyter-server 1.16.0\n",
      "    Uninstalling jupyter-server-1.16.0:\n",
      "      Successfully uninstalled jupyter-server-1.16.0\n",
      "Successfully installed arrow-1.3.0 async-lru-2.0.5 fqdn-1.5.1 isoduration-20.11.0 jedi-0.19.2 json5-0.12.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.16.0 jupyter-server-terminals-0.5.3 jupyterlab-4.4.3 jupyterlab-server-2.27.3 overrides-7.7.0 python-json-logger-3.3.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 types-python-dateutil-2.9.0.20250516 uri-template-1.3.0\n",
      "✅ Dependências instaladas!\n",
      "📱 Executando no Google Colab\n"
     ]
    }
   ],
   "source": [
    "# === INSTALAÇÃO DE DEPENDÊNCIAS ===\n",
    "print(\"🔧 Instalando dependências necessárias...\")\n",
    "\n",
    "# Instalar braindecode e dependências científicas\n",
    "!pip install braindecode mne torch torchvision torchaudio\n",
    "!pip install scikit-learn pandas numpy matplotlib seaborn\n",
    "!pip install scipy jupyter\n",
    "\n",
    "print(\"✅ Dependências instaladas!\")\n",
    "\n",
    "# Verificar se está no Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"📱 Executando no Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"💻 Executando localmente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf49bd97",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cf49bd97",
    "outputId": "c1d8d813-0f5c-4df6-eda3-3babbd9b61dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Configurando para Google Colab...\n",
      "📂 Diretório de trabalho: /content\n",
      "📁 Estrutura criada!\n"
     ]
    }
   ],
   "source": [
    "# === CLONE DO REPOSITÓRIO (OPCIONAL) ===\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"🔄 Configurando para Google Colab...\")\n",
    "\n",
    "    # Se você quiser clonar seu repositório:\n",
    "    # !git clone https://github.com/seu-usuario/projetoBCI.git\n",
    "    # os.chdir('/content/projetoBCI')\n",
    "\n",
    "    # Por enquanto, vamos trabalhar no diretório padrão do Colab\n",
    "    project_root = Path('/content')\n",
    "\n",
    "    # Criar estrutura de diretórios\n",
    "    (Path(\"/content/drive/MyDrive/Colab Notebooks/eeg_data\")).mkdir(exist_ok=True)\n",
    "    (project_root / \"models\").mkdir(exist_ok=True)\n",
    "    (project_root / \"results\").mkdir(exist_ok=True)\n",
    "\n",
    "else:\n",
    "    # Executando localmente\n",
    "    project_root = Path(os.getcwd()).parent\n",
    "\n",
    "print(f\"📂 Diretório de trabalho: {project_root}\")\n",
    "print(f\"📁 Estrutura criada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c91b8",
   "metadata": {
    "id": "244c91b8"
   },
   "source": [
    "## 2. Implementação das Classes Base (Inline)\n",
    "\n",
    "Já que não temos o repositório clonado, vamos implementar as classes principais inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4587fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e4587fa",
    "outputId": "e22d80a4-c3c3-4657-f57b-6a3b4eebd828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Imports básicos carregados!\n"
     ]
    }
   ],
   "source": [
    "# === IMPLEMENTAÇÃO INLINE DAS CLASSES BASE ===\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from typing import Optional, List, Tuple, Dict, Union\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ Imports básicos carregados!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7fe101",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a7fe101",
    "outputId": "04ea0cdc-4a63-4d5b-8552-76dae598e233"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ UniversalEEGNormalizer implementado!\n"
     ]
    }
   ],
   "source": [
    "# === UNIVERSAL EEG NORMALIZER ===\n",
    "class UniversalEEGNormalizer:\n",
    "    \"\"\"Normalizador universal para dados EEG\"\"\"\n",
    "\n",
    "    def __init__(self, method: str = 'zscore', mode: str = 'training'):\n",
    "        self.method = method\n",
    "        self.mode = mode\n",
    "        self.global_stats = {}\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def _ensure_3d(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Garantir que dados estejam em 3D (n_samples, n_channels, n_timepoints)\"\"\"\n",
    "        if len(data.shape) == 2:\n",
    "            n_samples, n_features = data.shape\n",
    "            if n_features % 16 == 0:  # Assumir 16 canais\n",
    "                n_channels = 16\n",
    "                n_timepoints = n_features // n_channels\n",
    "                data = data.reshape(n_samples, n_channels, n_timepoints)\n",
    "            else:\n",
    "                data = data[:, np.newaxis, :]\n",
    "        elif len(data.shape) == 1:\n",
    "            data = data[np.newaxis, np.newaxis, :]\n",
    "        return data\n",
    "\n",
    "    def fit(self, data: np.ndarray):\n",
    "        \"\"\"Ajustar normalizador aos dados\"\"\"\n",
    "        data_3d = self._ensure_3d(data)\n",
    "\n",
    "        if self.method == 'zscore':\n",
    "            self.global_stats['mean'] = np.mean(data_3d, axis=(0, 2), keepdims=True)\n",
    "            self.global_stats['std'] = np.std(data_3d, axis=(0, 2), keepdims=True)\n",
    "            self.global_stats['std'] = np.where(self.global_stats['std'] == 0, 1.0, self.global_stats['std'])\n",
    "\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Transformar dados\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Normalizador deve ser ajustado antes da transformação\")\n",
    "\n",
    "        original_shape = data.shape\n",
    "        data_3d = self._ensure_3d(data)\n",
    "\n",
    "        if self.method == 'zscore':\n",
    "            normalized = (data_3d - self.global_stats['mean']) / self.global_stats['std']\n",
    "\n",
    "        # Restaurar forma original\n",
    "        if len(original_shape) == 2:\n",
    "            return normalized.reshape(original_shape[0], -1)\n",
    "        elif len(original_shape) == 1:\n",
    "            return normalized.flatten()\n",
    "        return normalized\n",
    "\n",
    "    def fit_transform(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Ajustar e transformar em um passo\"\"\"\n",
    "        return self.fit(data).transform(data)\n",
    "\n",
    "print(\"✅ UniversalEEGNormalizer implementado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d3f933",
   "metadata": {
    "id": "80d3f933"
   },
   "source": [
    "# Normalização Melhorada para EEG\n",
    "\n",
    "Implementação de um normalizador mais robusto com:\n",
    "- Múltiplas estratégias de normalização\n",
    "- Validação de qualidade\n",
    "- Tratamento de outliers\n",
    "- Normalização por canal ou por trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40143506",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40143506",
    "outputId": "eae1e7a3-aaa7-4d4a-c420-945b281fe754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Normalizador melhorado implementado!\n"
     ]
    }
   ],
   "source": [
    "# === IMPROVED EEG NORMALIZER ===\n",
    "import numpy as np\n",
    "from typing import Dict, Optional, Tuple\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "class ImprovedEEGNormalizer:\n",
    "    \"\"\"Normalizador EEG avançado com múltiplas estratégias e validação\"\"\"\n",
    "\n",
    "    def __init__(self, method: str = 'robust_zscore', scope: str = 'channel',\n",
    "                 outlier_threshold: float = 3.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            method: 'robust_zscore', 'minmax', ou 'raw_zscore'\n",
    "            scope: 'channel', 'trial', ou 'global'\n",
    "            outlier_threshold: número de desvios para considerar outlier\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.scope = scope\n",
    "        self.outlier_threshold = outlier_threshold\n",
    "        self.stats: Dict = {}\n",
    "        self.is_fitted = False\n",
    "\n",
    "    def _handle_outliers(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Detecta e trata outliers usando IQR ou desvio padrão\"\"\"\n",
    "        if self.method == 'robust_zscore':\n",
    "            Q1 = np.percentile(X, 25, axis=(0, 2), keepdims=True)\n",
    "            Q3 = np.percentile(X, 75, axis=(0, 2), keepdims=True)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - self.outlier_threshold * IQR\n",
    "            upper = Q3 + self.outlier_threshold * IQR\n",
    "        else:\n",
    "            mean = np.mean(X, axis=(0, 2), keepdims=True)\n",
    "            std = np.std(X, axis=(0, 2), keepdims=True)\n",
    "            lower = mean - self.outlier_threshold * std\n",
    "            upper = mean + self.outlier_threshold * std\n",
    "\n",
    "        # Clip valores extremos\n",
    "        return np.clip(X, lower, upper)\n",
    "\n",
    "    def fit(self, X: np.ndarray) -> 'ImprovedEEGNormalizer':\n",
    "        \"\"\"Ajusta o normalizador aos dados\n",
    "\n",
    "        Args:\n",
    "            X: Array (trials, channels, time) ou (trials, features)\n",
    "        \"\"\"\n",
    "        # Garantir formato 3D\n",
    "        if len(X.shape) == 2:\n",
    "            if X.shape[1] % 16 == 0:  # Assumir 16 canais\n",
    "                n_channels = 16\n",
    "                X = X.reshape(X.shape[0], n_channels, -1)\n",
    "            else:\n",
    "                X = X[:, np.newaxis, :]\n",
    "\n",
    "        # Tratar outliers\n",
    "        X = self._handle_outliers(X)\n",
    "\n",
    "        if self.scope == 'channel':\n",
    "            if self.method == 'robust_zscore':\n",
    "                self.stats['median'] = np.median(X, axis=(0, 2), keepdims=True)\n",
    "                q75, q25 = np.percentile(X, [75, 25], axis=(0, 2))\n",
    "                self.stats['iqr'] = (q75 - q25)[None, :, None] + 1e-8\n",
    "\n",
    "            elif self.method == 'minmax':\n",
    "                self.stats['min'] = X.min(axis=(0, 2), keepdims=True)\n",
    "                self.stats['max'] = X.max(axis=(0, 2), keepdims=True)\n",
    "\n",
    "            else:  # raw_zscore\n",
    "                self.stats['mean'] = np.mean(X, axis=(0, 2), keepdims=True)\n",
    "                self.stats['std'] = np.std(X, axis=(0, 2), keepdims=True) + 1e-8\n",
    "\n",
    "        elif self.scope == 'trial':\n",
    "            if self.method == 'robust_zscore':\n",
    "                self.stats['median'] = np.median(X, axis=2, keepdims=True)\n",
    "                q75, q25 = np.percentile(X, [75, 25], axis=2)\n",
    "                self.stats['iqr'] = (q75 - q25)[:, :, None] + 1e-8\n",
    "\n",
    "            elif self.method == 'minmax':\n",
    "                self.stats['min'] = X.min(axis=2, keepdims=True)\n",
    "                self.stats['max'] = X.max(axis=2, keepdims=True)\n",
    "\n",
    "            else:  # raw_zscore\n",
    "                self.stats['mean'] = np.mean(X, axis=2, keepdims=True)\n",
    "                self.stats['std'] = np.std(X, axis=2, keepdims=True) + 1e-8\n",
    "\n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Transforma os dados usando as estatísticas calculadas\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Normalize.fit() deve ser chamado antes de transform()\")\n",
    "\n",
    "        # Garantir formato 3D\n",
    "        original_shape = X.shape\n",
    "        if len(X.shape) == 2:\n",
    "            if X.shape[1] % 16 == 0:\n",
    "                n_channels = 16\n",
    "                X = X.reshape(X.shape[0], n_channels, -1)\n",
    "            else:\n",
    "                X = X[:, np.newaxis, :]\n",
    "\n",
    "        # Transformação\n",
    "        if self.method == 'robust_zscore':\n",
    "            X_norm = (X - self.stats['median']) / self.stats['iqr']\n",
    "        elif self.method == 'minmax':\n",
    "            X_norm = (X - self.stats['min']) / (self.stats['max'] - self.stats['min'] + 1e-8)\n",
    "        else:  # raw_zscore\n",
    "            X_norm = (X - self.stats['mean']) / self.stats['std']\n",
    "\n",
    "        # Restaurar forma original se necessário\n",
    "        if len(original_shape) == 2:\n",
    "            X_norm = X_norm.reshape(original_shape)\n",
    "\n",
    "        return X_norm\n",
    "\n",
    "    def fit_transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Ajusta aos dados e transforma em um único passo\"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Retorna estatísticas de normalização\"\"\"\n",
    "        return self.stats.copy()\n",
    "\n",
    "\n",
    "def validate_normalization(X: np.ndarray, normalized_X: np.ndarray) -> Tuple[bool, Dict]:\n",
    "    \"\"\"Valida a qualidade da normalização\n",
    "\n",
    "    Args:\n",
    "        X: Dados originais\n",
    "        normalized_X: Dados normalizados\n",
    "\n",
    "    Returns:\n",
    "        (is_valid, stats): Booleano indicando se passou nos checks e estatísticas\n",
    "    \"\"\"\n",
    "    stats = {}\n",
    "\n",
    "    # 1. Verificar média e desvio\n",
    "    stats['mean'] = float(np.mean(normalized_X))\n",
    "    stats['std'] = float(np.std(normalized_X))\n",
    "\n",
    "    # 2. Calcular percentis\n",
    "    stats['percentiles'] = {\n",
    "        '1%': float(np.percentile(normalized_X, 1)),\n",
    "        '99%': float(np.percentile(normalized_X, 99))\n",
    "    }\n",
    "\n",
    "    # 3. Verificar preservação de ordem relativa\n",
    "    original_order = np.argsort(np.mean(X, axis=(1,2)))\n",
    "    normalized_order = np.argsort(np.mean(normalized_X, axis=(1,2)))\n",
    "    stats['order_correlation'] = float(np.corrcoef(original_order, normalized_order)[0,1])\n",
    "\n",
    "    # 4. Verificar outliers\n",
    "    z_scores = np.abs((normalized_X - np.mean(normalized_X)) / np.std(normalized_X))\n",
    "    stats['outliers_ratio'] = float(np.mean(z_scores > 3))\n",
    "\n",
    "    # Critérios de validação\n",
    "    is_valid = (\n",
    "        abs(stats['mean']) < 0.1 and           # Média próxima de zero\n",
    "        abs(stats['std'] - 1) < 0.5 and        # Desvio próximo de 1\n",
    "        stats['order_correlation'] > 0.7 and    # Preserva ordem relativa\n",
    "        stats['outliers_ratio'] < 0.01         # Menos de 1% outliers\n",
    "    )\n",
    "\n",
    "    return is_valid, stats\n",
    "\n",
    "print(\"✅ Normalizador melhorado implementado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e6ebcc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12e6ebcc",
    "outputId": "4a612254-1e74-4d2d-80bb-befc0d076b74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BCIDataset implementado!\n"
     ]
    }
   ],
   "source": [
    "# === BCI DATASET CLASS ===\n",
    "class BCIDataset(Dataset):\n",
    "    \"\"\"Dataset PyTorch para dados EEG\"\"\"\n",
    "\n",
    "    def __init__(self, windows: np.ndarray, labels: np.ndarray, transform=None, augment: bool = False):\n",
    "        self.windows = torch.from_numpy(windows).float()\n",
    "        self.labels = torch.from_numpy(labels).long()\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window = self.windows[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.augment:\n",
    "            # Adicionar ruído leve\n",
    "            if torch.rand(1) < 0.3:\n",
    "                noise = torch.randn_like(window) * 0.01\n",
    "                window = window + noise\n",
    "\n",
    "        if self.transform:\n",
    "            window = self.transform(window)\n",
    "\n",
    "        return window, label\n",
    "\n",
    "print(\"✅ BCIDataset implementado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4ebbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bec4ebbc",
    "outputId": "37963b05-791a-4db7-e486-c99863101af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Braindecode EEGInceptionERP importado com sucesso!\n",
      "✅ EEGInceptionERPModel implementado! Tipo: Braindecode\n"
     ]
    }
   ],
   "source": [
    "# === EEG INCEPTION ERP MODEL ===\n",
    "try:\n",
    "    from braindecode.models import EEGInceptionERP\n",
    "    print(\"✅ Braindecode EEGInceptionERP importado com sucesso!\")\n",
    "    BRAINDECODE_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Erro ao importar braindecode: {e}\")\n",
    "    print(\"🔧 Vamos implementar um modelo CNN simples como fallback\")\n",
    "    BRAINDECODE_AVAILABLE = False\n",
    "\n",
    "class EEGInceptionERPModel(nn.Module):\n",
    "    \"\"\"Wrapper para EEGInceptionERP com fallback para CNN simples\"\"\"\n",
    "\n",
    "    def __init__(self, n_chans: int, n_outputs: int, n_times: int, sfreq: float = 125.0, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_chans = n_chans\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_times = n_times\n",
    "        self.sfreq = sfreq\n",
    "        self.is_trained = False\n",
    "\n",
    "        if BRAINDECODE_AVAILABLE:\n",
    "            try:\n",
    "                self.model = EEGInceptionERP(\n",
    "                    n_chans=n_chans,\n",
    "                    n_outputs=n_outputs,\n",
    "                    n_times=n_times,\n",
    "                    sfreq=sfreq\n",
    "                )\n",
    "                self.model_type = \"EEGInceptionERP\"\n",
    "                print(f\"✅ Usando EEGInceptionERP: {n_chans} canais, {n_times} pontos temporais\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Erro ao criar EEGInceptionERP: {e}\")\n",
    "                self.model = self._create_fallback_cnn(n_chans, n_outputs, n_times)\n",
    "                self.model_type = \"FallbackCNN\"\n",
    "        else:\n",
    "            self.model = self._create_fallback_cnn(n_chans, n_outputs, n_times)\n",
    "            self.model_type = \"FallbackCNN\"\n",
    "\n",
    "    def _create_fallback_cnn(self, n_chans: int, n_outputs: int, n_times: int):\n",
    "        \"\"\"Criar CNN simples como fallback\"\"\"\n",
    "        print(f\"🔧 Criando CNN fallback: {n_chans} canais, {n_times} pontos temporais\")\n",
    "        return nn.Sequential(\n",
    "            # Convolução temporal\n",
    "            nn.Conv1d(n_chans, 32, kernel_size=25, padding=12),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Redução de dimensionalidade\n",
    "            nn.Conv1d(32, 64, kernel_size=15, padding=7),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(4),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Convolução final\n",
    "            nn.Conv1d(64, 128, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # Classificador\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, n_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.model_type == \"EEGInceptionERP\":\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            # Para o CNN fallback, precisamos transpor de (batch, channels, time) para (batch, channels, time)\n",
    "            return self.model(x)\n",
    "\n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Salvar modelo\"\"\"\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        save_dict = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'model_type': self.model_type,\n",
    "            'n_chans': self.n_chans,\n",
    "            'n_outputs': self.n_outputs,\n",
    "            'n_times': self.n_times,\n",
    "            'sfreq': self.sfreq,\n",
    "            'is_trained': self.is_trained\n",
    "        }\n",
    "        torch.save(save_dict, filepath)\n",
    "        print(f\"✅ Modelo salvo: {filepath}\")\n",
    "\n",
    "    def load(self, filepath: str):\n",
    "        \"\"\"Carregar modelo\"\"\"\n",
    "        checkpoint = torch.load(filepath, map_location='cpu')\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.is_trained = checkpoint.get('is_trained', True)\n",
    "        print(f\"✅ Modelo carregado: {filepath}\")\n",
    "\n",
    "print(f\"✅ EEGInceptionERPModel implementado! Tipo: {'Braindecode' if BRAINDECODE_AVAILABLE else 'Fallback CNN'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099f820",
   "metadata": {
    "id": "3099f820"
   },
   "source": [
    "## 3. Configuração dos Caminhos e Parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8071299",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8071299",
    "outputId": "8842b224-2b7b-4df3-e645-71fa0185ea77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configurações definidas:\n",
      "📂 Diretório de dados: /content/drive/MyDrive/Colab Notebooks/eeg_data/MNE-eegbci-data/files/eegmmidb/1.0.0\n",
      "🏗️ Modelos: /content/drive/MyDrive/Colab Notebooks/eeg_data/models\n",
      "📊 Resultados: /content/drive/MyDrive/Colab Notebooks/eeg_data/results\n",
      "\n",
      "📝 Parâmetros principais:\n",
      "  - exclude_subjects: ['Davi']\n",
      "  - data_base_path: /content/drive/MyDrive/Colab Notebooks/eeg_data/MNE-eegbci-data/files/eegmmidb/1.0.0\n",
      "  - model_save_path: /content/drive/MyDrive/Colab Notebooks/eeg_data/models\n",
      "  - results_save_path: /content/drive/MyDrive/Colab Notebooks/eeg_data/results\n",
      "  - num_k_folds: 10\n",
      "  - num_epochs_per_fold: 30\n",
      "  - batch_size: 10\n",
      "  - early_stopping_patience: 8\n",
      "  - learning_rate: 0.001\n",
      "  - test_split_ratio: 0.2\n",
      "  - model_name: eeg_inception_openbci_cv10\n",
      "  normalization:\n",
      "    - method: robust_zscore\n",
      "    - scope: channel\n",
      "    - outlier_threshold: 3.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# --- Raiz do seu trabalho no Drive ------------------------------------------------\n",
    "PROJECT_ROOT = Path(\"/content/drive/MyDrive/Colab Notebooks/eeg_data\")\n",
    "\n",
    "# --- Dados ------------------------------------------------------------------------\n",
    "EEG_DATA_PATH  = PROJECT_ROOT / \"MNE-eegbci-data/files/eegmmidb/1.0.0\"\n",
    "DAVI_DATA_PATH = EEG_DATA_PATH / \"Davi\"\n",
    "\n",
    "# --- Saída de modelos e resultados ------------------------------------------------\n",
    "MODELS_PATH  = PROJECT_ROOT / \"models\"     # <-- ajuste se quiser outro local\n",
    "RESULTS_PATH = PROJECT_ROOT / \"results\"\n",
    "\n",
    "# Cria as pastas se ainda não existirem (evita erros de salvamento depois)\n",
    "MODELS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Parâmetros de treinamento ---------------------------------------------------\n",
    "TRAINING_PARAMS = {\n",
    "    # Configurações de dados\n",
    "    'exclude_subjects': ['Davi'],  # treina com todos exceto Davi\n",
    "    'data_base_path': str(EEG_DATA_PATH),\n",
    "    'model_save_path': str(MODELS_PATH),\n",
    "    'results_save_path': str(RESULTS_PATH),\n",
    "\n",
    "    # Configurações de treinamento\n",
    "    'num_k_folds': 10,            # Número de folds para validação cruzada\n",
    "    'num_epochs_per_fold': 30,    # Épocas máximas por fold\n",
    "    'batch_size': 10,             # Tamanho do batch\n",
    "    'early_stopping_patience': 8,  # Paciência para early stopping\n",
    "    'learning_rate': 1e-3,        # Taxa de aprendizado\n",
    "    'test_split_ratio': 0.2,      # Proporção do conjunto de teste\n",
    "\n",
    "    # Identificação do modelo\n",
    "    'model_name': 'eeg_inception_openbci_cv10',\n",
    "\n",
    "    # Configurações de normalização\n",
    "    'normalization': {\n",
    "        'method': 'robust_zscore',\n",
    "        'scope': 'channel',\n",
    "        'outlier_threshold': 3.0\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Configurações definidas:\")\n",
    "print(f\"📂 Diretório de dados: {EEG_DATA_PATH}\")\n",
    "print(f\"🏗️ Modelos: {MODELS_PATH}\")\n",
    "print(f\"📊 Resultados: {RESULTS_PATH}\")\n",
    "print(\"\\n📝 Parâmetros principais:\")\n",
    "for key, value in TRAINING_PARAMS.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  {key}:\")\n",
    "        for k, v in value.items():\n",
    "            print(f\"    - {k}: {v}\")\n",
    "    else:\n",
    "        print(f\"  - {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4779d717",
   "metadata": {
    "id": "4779d717"
   },
   "source": [
    "## 4. Geração de Dados Sintéticos para Teste\n",
    "\n",
    "Já que provavelmente você não tem os dados reais no Colab, vamos gerar dados sintéticos para testar o pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rj2HrT2WASE0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rj2HrT2WASE0",
    "outputId": "1571f815-09c1-4729-e1b2-419681f9dd15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Carregando dataset OpenBCI CSV...\n",
      "✅ Dados carregados: (3530, 16, 401) janelas – [1781 1749] (classes) – 79 sujeitos\n"
     ]
    }
   ],
   "source": [
    "# === CARREGAMENTO DE DADOS REAIS (OpenBCI CSV) ===\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "import re\n",
    "\n",
    "def _read_openbci_csv(file_path: Path, sfreq: int = 125):\n",
    "    \"\"\"Carrega um único arquivo CSV gerado pela OpenBCI GUI e devolve um Raw MNE.\n",
    "    Aprende automaticamente os nomes dos canais EXG… presentes no cabeçalho.\n",
    "    Adiciona anotações (T0, T1, T2…) encontradas na coluna *Annotations*.\n",
    "    \"\"\"\n",
    "    # Lê o CSV ignorando linhas iniciadas com '%'\n",
    "    df = pd.read_csv(file_path, comment='%')\n",
    "\n",
    "    # Seleciona colunas de EEG (por padrão 'EXG Channel 0‑15')\n",
    "    eeg_cols = [c for c in df.columns if c.lower().startswith('exg channel')]\n",
    "    data = df[eeg_cols].T.to_numpy() * 1e-6  # µV → V\n",
    "\n",
    "    info = mne.create_info(ch_names=eeg_cols, sfreq=sfreq, ch_types='eeg')\n",
    "    raw = mne.io.RawArray(data, info, verbose=False)\n",
    "\n",
    "    # Anotações: mapeia strings 'T0','T1','T2'… para eventos\n",
    "    if 'Annotations' in df.columns:\n",
    "        onsets = df.index.to_numpy() / sfreq\n",
    "        for onset, annot in zip(onsets, df['Annotations'].astype(str)):\n",
    "            if annot.startswith('T'):\n",
    "                raw.annotations.append(onset, 0, annot)\n",
    "    return raw\n",
    "\n",
    "def load_openbci_eeg(data_dir: Path):\n",
    "    \"\"\"Carrega janelas EEG (trials × canais × tempo) e rótulos binários\n",
    "    (0: mão esquerda, 1: mão direita) a partir de arquivos CSV da OpenBCI.\n",
    "\n",
    "    Arquitetura esperada:\n",
    "        data_dir/\n",
    "            S001R01_csv_openbci.csv\n",
    "            S001R02_csv_openbci.csv\n",
    "            ...\n",
    "            Davi/\n",
    "                Davi_R01_csv_openbci.csv\n",
    "                ...\n",
    "    O mapeamento de rótulos assume:\n",
    "        'T1' → 0 (mão esquerda)\n",
    "        'T2' → 1 (mão direita)\n",
    "    Ajuste conforme a sua anotação real, se necessário.\n",
    "    \"\"\"\n",
    "    windows, labels, subject_ids = [], [], []\n",
    "\n",
    "    for csv_file in sorted(data_dir.rglob('*_csv_openbci.csv')):\n",
    "        # Infer subject ID (primeiros 3 dígitos após 'S')\n",
    "        match = re.search(r'[Ss](\\d{3})', csv_file.stem)\n",
    "        subj_id = int(match.group(1)) if match else 0\n",
    "\n",
    "        raw = _read_openbci_csv(csv_file)\n",
    "\n",
    "        # Extrai eventos T1/T2\n",
    "        events, event_id = mne.events_from_annotations(raw,\n",
    "                                                       event_id={'T1': 1, 'T2': 2},\n",
    "                                                       verbose=False)\n",
    "        if len(events) == 0:\n",
    "            continue\n",
    "\n",
    "        # Epoca intervalo 0‑3.2 s (≈ 400 amostras a 125 Hz)\n",
    "        epochs = mne.Epochs(raw,\n",
    "                            events,\n",
    "                            event_id=event_id,\n",
    "                            tmin=0,\n",
    "                            tmax=3.2,\n",
    "                            baseline=None,\n",
    "                            preload=True,\n",
    "                            verbose=False)\n",
    "\n",
    "        lbl = epochs.events[:, 2] - 1  # Converte 1→0 (esq.), 2→1 (dir.)\n",
    "        windows.append(epochs.get_data())\n",
    "        labels.append(lbl)\n",
    "        subject_ids.append(np.full(len(lbl), subj_id))\n",
    "\n",
    "    if not windows:\n",
    "        raise RuntimeError(\"Nenhuma janela EEG encontrada — verifique suas anotações T1/T2.\")\n",
    "\n",
    "    windows = np.concatenate(windows, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    subject_ids = np.concatenate(subject_ids, axis=0)\n",
    "    return windows, labels, subject_ids\n",
    "\n",
    "\n",
    "print(\"📥 Carregando dataset OpenBCI CSV...\")\n",
    "windows, labels, subject_ids = load_openbci_eeg(EEG_DATA_PATH)\n",
    "print(f\"✅ Dados carregados: {windows.shape} janelas – {np.bincount(labels)} (classes) – {len(np.unique(subject_ids))} sujeitos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837787c",
   "metadata": {
    "id": "c837787c"
   },
   "source": [
    "## 5. Implementação do Pipeline de Treinamento Simplificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b93b1",
   "metadata": {
    "id": "304b93b1"
   },
   "outputs": [],
   "source": [
    "### Parâmetros de treinamento\n",
    "TRAINING_PARAMS = {\n",
    "    'exclude_subjects': ['Davi'],  # treina com todos exceto Davi\n",
    "    'num_k_folds': 10,\n",
    "    'num_epochs_per_fold': 30,\n",
    "    'batch_size': 10,\n",
    "    'early_stopping_patience': 8,\n",
    "    'learning_rate': 1e-3,\n",
    "    'test_split_ratio': 0.2,\n",
    "    'model_name': 'eeg_inception_openbci_cv10',\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# Dependências\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# =============================================================================\n",
    "# Função principal de treinamento\n",
    "# =============================================================================\n",
    "\n",
    "def train_simplified_model(windows, labels, subject_ids, params):\n",
    "    \"\"\"Pipeline de treinamento simplificado com CV e early stopping.\"\"\"\n",
    "\n",
    "    print('🚀 Iniciando treinamento simplificado…')\n",
    "\n",
    "    # --- Excluir sujeitos ------------------------------------------------------\n",
    "    if isinstance(params.get('exclude_subjects'), list):\n",
    "        mask = ~np.isin(subject_ids, params['exclude_subjects'])\n",
    "        windows, labels, subject_ids = windows[mask], labels[mask], subject_ids[mask]\n",
    "\n",
    "    print(f\"📊 Amostras após exclusão: {windows.shape[0]}\")\n",
    "\n",
    "    # --- Normalização ----------------------------------------------------------\n",
    "    print(\"\\n📊 Configurando normalização...\")\n",
    "    normalizer = ImprovedEEGNormalizer(\n",
    "        method='robust_zscore',  # mais robusto a outliers\n",
    "        scope='channel',         # normaliza por canal\n",
    "        outlier_threshold=3.0    # 3 desvios padrão para outliers\n",
    "    )\n",
    "\n",
    "    # Normalizar dados\n",
    "    windows_norm = normalizer.fit_transform(windows)\n",
    "\n",
    "    # Validar normalização\n",
    "    is_valid, norm_stats = validate_normalization(windows, windows_norm)\n",
    "    print(\"\\n🔍 Validação da Normalização:\")\n",
    "    print(f\"  - Média: {norm_stats['mean']:.3f} (ideal: próximo de 0)\")\n",
    "    print(f\"  - Desvio: {norm_stats['std']:.3f} (ideal: próximo de 1)\")\n",
    "    print(f\"  - Correlação de ordem: {norm_stats['order_correlation']:.3f}\")\n",
    "    print(f\"  - Proporção de outliers: {norm_stats['outliers_ratio']*100:.2f}%\")\n",
    "    print(f\"  - Status: {'✅ OK' if is_valid else '⚠️ Verificar'}\")\n",
    "\n",
    "    # --- Split treino/val/test -------------------------------------------------\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        windows_norm, labels,\n",
    "        test_size=params['test_split_ratio'],\n",
    "        random_state=42,\n",
    "        stratify=labels\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\n🖥️ Device: {device}\")\n",
    "\n",
    "    # --- Validação cruzada -----------------------------------------------------\n",
    "    kfold = KFold(n_splits=params['num_k_folds'], shuffle=True, random_state=42)\n",
    "    fold_accs = []\n",
    "    best_model = None\n",
    "    best_overall_acc = 0.0\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_val), 1):\n",
    "        print(f\"\\n📁 Fold {fold}/{params['num_k_folds']}\")\n",
    "\n",
    "        X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
    "        y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
    "\n",
    "        # Usar augmentation apenas no treino\n",
    "        train_ds = BCIDataset(X_train, y_train, augment=True)\n",
    "        val_ds   = BCIDataset(X_val,   y_val,   augment=False)\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=params['batch_size'], shuffle=True)\n",
    "        val_loader   = DataLoader(val_ds,   batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "        model = EEGInceptionERPModel(\n",
    "            n_chans=windows.shape[1],\n",
    "            n_outputs=len(np.unique(labels)),\n",
    "            n_times=windows.shape[2]\n",
    "        ).to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optim     = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "        best_val_acc, patience = 0.0, 0\n",
    "        fold_history = []\n",
    "\n",
    "        for epoch in range(params['num_epochs_per_fold']):\n",
    "            # ---- Treinamento ----\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            correct = total = 0\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optim.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                pred = out.argmax(1)\n",
    "                correct += (pred == yb).sum().item()\n",
    "                total   += yb.size(0)\n",
    "\n",
    "            train_acc = correct / total\n",
    "            train_loss = train_loss / len(train_loader)\n",
    "\n",
    "            # ---- Validação ----\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = total = 0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    out = model(xb)\n",
    "                    val_loss += criterion(out, yb).item()\n",
    "                    pred = out.argmax(1)\n",
    "                    correct += (pred == yb).sum().item()\n",
    "                    total   += yb.size(0)\n",
    "\n",
    "            val_acc = correct / total\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "\n",
    "            fold_history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_acc': train_acc,\n",
    "                'train_loss': train_loss,\n",
    "                'val_acc': val_acc,\n",
    "                'val_loss': val_loss\n",
    "            })\n",
    "\n",
    "            print(f\"  Epoch {epoch+1}: Train {train_acc:.3f} ({train_loss:.3f}) | Val {val_acc:.3f} ({val_loss:.3f})\")\n",
    "\n",
    "            # ---- Early stopping ----\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience = 0\n",
    "                # Guardar melhor modelo geral\n",
    "                if val_acc > best_overall_acc:\n",
    "                    best_overall_acc = val_acc\n",
    "                    best_model = model.state_dict()\n",
    "            else:\n",
    "                patience += 1\n",
    "                if patience >= params['early_stopping_patience']:\n",
    "                    print(f\"  ⏹️  Early stopping na época {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        fold_accs.append(best_val_acc)\n",
    "        print(f\"✅ Fold {fold} concluído – Melhor Val Acc: {best_val_acc:.4f}\")\n",
    "\n",
    "    # --- Avaliação no conjunto de teste ----------------------------------------\n",
    "    print(\"\\n🎯 Avaliando no conjunto de teste...\")\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "        test_ds = BCIDataset(X_test, y_test, augment=False)\n",
    "        test_loader = DataLoader(test_ds, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        correct = total = 0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                test_loss += criterion(out, yb).item()\n",
    "                pred = out.argmax(1)\n",
    "                correct += (pred == yb).sum().item()\n",
    "                total += yb.size(0)\n",
    "\n",
    "                predictions.extend(pred.cpu().numpy())\n",
    "                true_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "        test_acc = correct / total\n",
    "        test_loss = test_loss / len(test_loader)\n",
    "    else:\n",
    "        test_acc = test_loss = 0.0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "    # Preparar resultados estruturados\n",
    "    cv_mean = np.mean(fold_accs)\n",
    "    cv_std = np.std(fold_accs)\n",
    "    print(f\"\\n📊 Resultados Finais:\")\n",
    "    print(f\"  CV Mean Acc: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "    print(f\"  Test Acc: {test_acc:.4f} (loss: {test_loss:.4f})\")\n",
    "\n",
    "    results = {\n",
    "        'fold_accuracies': fold_accs,\n",
    "        'cv_mean_accuracy': cv_mean,\n",
    "        'cv_std_accuracy': cv_std,\n",
    "        'final_test_accuracy': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'model_name': params['model_name'],\n",
    "        'best_model_state': best_model,\n",
    "        'normalization_stats': normalizer.get_stats(),\n",
    "        'normalization_validation': norm_stats,\n",
    "        'training_params': params,\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bc5b61",
   "metadata": {
    "id": "41bc5b61"
   },
   "source": [
    "## 6. Execução do Treinamento Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3b1eb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1e3b1eb",
    "outputId": "e417cc0f-4092-46d9-aeea-54ef7eaeac0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 INICIANDO TREINAMENTO PRINCIPAL...\n",
      "❌ Erro durante treinamento: name 'MAIN_TRAINING_PARAMS' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-12-1593755867>\", line 10, in <cell line: 0>\n",
      "    MAIN_TRAINING_PARAMS\n",
      "NameError: name 'MAIN_TRAINING_PARAMS' is not defined\n"
     ]
    }
   ],
   "source": [
    "# === EXECUÇÃO DO TREINAMENTO ===\n",
    "print(\"🚀 INICIANDO TREINAMENTO PRINCIPAL...\")\n",
    "\n",
    "try:\n",
    "    # Executar treinamento com dados sintéticos\n",
    "    main_results = train_simplified_model(\n",
    "        windows,\n",
    "        labels,\n",
    "        subject_ids,\n",
    "        MAIN_TRAINING_PARAMS\n",
    "    )\n",
    "\n",
    "    print(\"\\n📊 RESULTADOS DO TREINAMENTO:\")\n",
    "    print(f\"  - CV Mean: {main_results['cv_mean_accuracy']:.4f} ± {main_results['cv_std_accuracy']:.4f}\")\n",
    "    print(f\"  - Test Acc: {main_results['final_test_accuracy']:.4f}\")\n",
    "    print(f\"  - Modelo: {main_results['model_name']}\")\n",
    "\n",
    "    # Salvar resultados\n",
    "    results_path = RESULTS_PATH / \"main_training_results.json\"\n",
    "    RESULTS_PATH.mkdir(exist_ok=True)\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(main_results, f, indent=2, default=str)\n",
    "\n",
    "    print(f\"💾 Resultados salvos em: {results_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro durante treinamento: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    main_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd11f40",
   "metadata": {
    "id": "9cd11f40"
   },
   "source": [
    "## 7. Visualização dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac95a0ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac95a0ce",
    "outputId": "24b9f6a6-61ac-4990-a8a8-4934ac29270d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Não há resultados para visualizar\n"
     ]
    }
   ],
   "source": [
    "# === VISUALIZAÇÃO DOS RESULTADOS ===\n",
    "if main_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle('Resultados do Treinamento BCI - Google Colab', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Acurácias por fold\n",
    "    ax1 = axes[0, 0]\n",
    "    fold_accs = main_results['fold_accuracies']\n",
    "    ax1.bar(range(1, len(fold_accs) + 1), fold_accs, alpha=0.7, color='skyblue')\n",
    "    ax1.axhline(y=main_results['cv_mean_accuracy'], color='red', linestyle='--',\n",
    "                label=f\"Mean: {main_results['cv_mean_accuracy']:.3f}\")\n",
    "    ax1.set_xlabel('Fold')\n",
    "    ax1.set_ylabel('Acurácia')\n",
    "    ax1.set_title('Acurácia por Fold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Comparação CV vs Test\n",
    "    ax2 = axes[0, 1]\n",
    "    models = ['CV Mean', 'Test Final']\n",
    "    accuracies = [main_results['cv_mean_accuracy'], main_results['final_test_accuracy']]\n",
    "    bars = ax2.bar(models, accuracies, color=['lightcoral', 'lightgreen'], alpha=0.8)\n",
    "    ax2.set_ylabel('Acurácia')\n",
    "    ax2.set_title('CV vs Test Performance')\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "    # 3. Distribuição das classes sintéticas\n",
    "    ax3 = axes[1, 0]\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    ax3.pie(counts, labels=[f'Classe {u}' for u in unique], autopct='%1.1f%%')\n",
    "    ax3.set_title('Distribuição de Classes')\n",
    "\n",
    "    # 4. Resumo textual\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    RESUMO DO TREINAMENTO\n",
    "\n",
    "    Dataset: Sintético\n",
    "    Subjects: {len(np.unique(subject_ids))}\n",
    "    Amostras: {len(labels)}\n",
    "\n",
    "    Modelo: {main_results['model_name']}\n",
    "    Tipo: {'EEGInceptionERP' if BRAINDECODE_AVAILABLE else 'CNN Fallback'}\n",
    "\n",
    "    Performance:\n",
    "    • CV Mean: {main_results['cv_mean_accuracy']:.4f}\n",
    "    • CV Std: {main_results['cv_std_accuracy']:.4f}\n",
    "    • Test Acc: {main_results['final_test_accuracy']:.4f}\n",
    "\n",
    "    Status: ✅ Sucesso\n",
    "    \"\"\"\n",
    "    ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes,\n",
    "             fontsize=10, verticalalignment='top', fontfamily='monospace')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_PATH / 'colab_training_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"✅ Visualização criada e salva!\")\n",
    "else:\n",
    "    print(\"❌ Não há resultados para visualizar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baca50ec",
   "metadata": {
    "id": "baca50ec"
   },
   "source": [
    "# 9. Fine-tuning com Dados Específicos\n",
    "\n",
    "Nesta seção, vamos realizar o fine-tuning do modelo treinado anteriormente usando os dados específicos localizados na pasta Davi.\n",
    "\n",
    "O processo de fine-tuning envolve:\n",
    "1. Carregar o melhor modelo do treinamento anterior\n",
    "2. Congelar parte das camadas iniciais (transfer learning)\n",
    "3. Treinar apenas as últimas camadas com os novos dados\n",
    "4. Validar a performance no conjunto de teste específico\n",
    "\n",
    "Este processo é especialmente útil quando temos um modelo base treinado em muitos sujeitos e queremos adaptá-lo para um sujeito específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa017e1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa017e1b",
    "outputId": "244b843a-90ce-4fd6-be5c-96cbe6ef75e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Carregando dados do Davi...\n",
      "📊 Dados carregados: 20 amostras\n",
      "❌ Modelo base não disponível para fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# === FINE-TUNING COM DADOS ESPECÍFICOS ===\n",
    "def fine_tune_model(base_model, windows, labels, params, original_norm_stats=None):\n",
    "    \"\"\"Realiza fine-tuning do modelo base com dados específicos.\"\"\"\n",
    "    print(\"🎯 Iniciando fine-tuning...\")\n",
    "\n",
    "    # --- Preparação dos dados ------------------------------------------------\n",
    "    if original_norm_stats and params.get('use_original_normalization', False):\n",
    "        print(\"📊 Usando normalização do treino original...\")\n",
    "        # Verificar quais estatísticas temos disponíveis\n",
    "        if 'median' in original_norm_stats and 'iqr' in original_norm_stats:\n",
    "            windows_norm = (windows - original_norm_stats['median']) / original_norm_stats['iqr']\n",
    "        elif 'mean' in original_norm_stats and 'std' in original_norm_stats:\n",
    "            windows_norm = (windows - original_norm_stats['mean']) / original_norm_stats['std']\n",
    "        else:\n",
    "            print(\"⚠️ Estatísticas de normalização original não reconhecidas, usando nova normalização...\")\n",
    "            normalizer = ImprovedEEGNormalizer(method='robust_zscore')\n",
    "            windows_norm = normalizer.fit_transform(windows)\n",
    "    else:\n",
    "        print(\"📊 Usando nova normalização específica...\")\n",
    "        normalizer = ImprovedEEGNormalizer(method='robust_zscore')\n",
    "        windows_norm = normalizer.fit_transform(windows)\n",
    "\n",
    "        # Comparar estatísticas se tivermos as originais\n",
    "        if original_norm_stats:\n",
    "            # Pegar estatísticas atuais\n",
    "            current_stats = normalizer.get_stats()\n",
    "\n",
    "            # Comparar estatísticas disponíveis\n",
    "            if 'median' in current_stats and 'median' in original_norm_stats:\n",
    "                median_diff = np.abs(current_stats['median'] - original_norm_stats['median']).mean()\n",
    "                iqr_diff = np.abs(current_stats['iqr'] - original_norm_stats['iqr']).mean()\n",
    "                print(f\"📈 Diferença média nas estatísticas (robust):\")\n",
    "                print(f\"   - Mediana: {median_diff:.6f}\")\n",
    "                print(f\"   - IQR: {iqr_diff:.6f}\")\n",
    "            elif 'mean' in current_stats and 'mean' in original_norm_stats:\n",
    "                mean_diff = np.abs(current_stats['mean'] - original_norm_stats['mean']).mean()\n",
    "                std_diff = np.abs(current_stats['std'] - original_norm_stats['std']).mean()\n",
    "                print(f\"📈 Diferença média nas estatísticas (zscore):\")\n",
    "                print(f\"   - Média: {mean_diff:.6f}\")\n",
    "                print(f\"   - Desvio: {std_diff:.6f}\")\n",
    "\n",
    "    # Validar normalização\n",
    "    is_valid, norm_stats = validate_normalization(windows, windows_norm)\n",
    "    print(\"\\n🔍 Validação da Normalização:\")\n",
    "    print(f\"  - Média: {norm_stats['mean']:.3f} (ideal: próximo de 0)\")\n",
    "    print(f\"  - Desvio: {norm_stats['std']:.3f} (ideal: próximo de 1)\")\n",
    "    print(f\"  - Correlação de ordem: {norm_stats['order_correlation']:.3f}\")\n",
    "    print(f\"  - Proporção de outliers: {norm_stats['outliers_ratio']*100:.2f}%\")\n",
    "    print(f\"  - Status: {'✅ OK' if is_valid else '⚠️ Verificar'}\")\n",
    "\n",
    "    # --- Split train/val/test -------------------------------------------------\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        windows_norm, labels,\n",
    "        test_size=0.3,\n",
    "        random_state=42,\n",
    "        stratify=labels\n",
    "    )\n",
    "\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp,\n",
    "        test_size=0.5,\n",
    "        random_state=42,\n",
    "        stratify=y_temp\n",
    "    )\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = BCIDataset(X_train, y_train, augment=True)\n",
    "    val_ds = BCIDataset(X_val, y_val, augment=False)\n",
    "    test_ds = BCIDataset(X_test, y_test, augment=False)\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=params['batch_size'], shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=params['batch_size'], shuffle=False)\n",
    "\n",
    "    # --- Preparação do modelo -----------------------------------------------\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = base_model.to(device)\n",
    "\n",
    "    # Congelar camadas iniciais conforme estratégia\n",
    "    if params['freeze_strategy'] == 'early':\n",
    "        frozen_count = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'encoder' in name or 'conv' in name:\n",
    "                param.requires_grad = False\n",
    "                frozen_count += 1\n",
    "        print(f\"\\n🧊 Camadas congeladas: {frozen_count}\")\n",
    "\n",
    "    # Otimizador com learning rate reduzido\n",
    "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(\n",
    "        trainable_params,\n",
    "        lr=params['learning_rate_ratio'] * params['learning_rate']\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # --- Fine-tuning -------------------------------------------------------\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience = params.get('early_stopping_patience', 5)\n",
    "    wait = 0\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(params['epochs']):\n",
    "        # Treino\n",
    "        model.train()\n",
    "        train_loss = train_correct = train_total = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            pred = out.argmax(1)\n",
    "            train_correct += (pred == yb).sum().item()\n",
    "            train_total += yb.size(0)\n",
    "\n",
    "        train_acc = train_correct / train_total\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Validação\n",
    "        model.eval()\n",
    "        val_loss = val_correct = val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                pred = out.argmax(1)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_correct += (pred == yb).sum().item()\n",
    "                val_total += yb.size(0)\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        # Guardar histórico\n",
    "        history.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        })\n",
    "\n",
    "        print(f\"Época {epoch+1}: Train {train_acc:.3f} ({train_loss:.3f}) | Val {val_acc:.3f} ({val_loss:.3f})\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"\\n⏹️ Early stopping na época {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # --- Avaliação final --------------------------------------------------\n",
    "    print(\"\\n🎯 Avaliando modelo final...\")\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = test_correct = test_total = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            pred = out.argmax(1)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_correct += (pred == yb).sum().item()\n",
    "            test_total += yb.size(0)\n",
    "\n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            true_labels.extend(yb.cpu().numpy())\n",
    "\n",
    "    test_acc = test_correct / test_total\n",
    "    test_loss = test_loss / len(test_loader)\n",
    "\n",
    "    results = {\n",
    "        'final_test_accuracy': test_acc,\n",
    "        'best_val_accuracy': best_val_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'model_state': best_model_state,\n",
    "        'history': history,\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels,\n",
    "        'normalization_validation': norm_stats\n",
    "    }\n",
    "\n",
    "    print(f\"\\n✨ Resultados do Fine-tuning:\")\n",
    "    print(f\"   - Melhor Val Acc: {best_val_acc:.4f}\")\n",
    "    print(f\"   - Test Acc: {test_acc:.4f}\")\n",
    "    print(f\"   - Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# === EXECUÇÃO DO FINE-TUNING ===\n",
    "print(\"🔄 Carregando dados do Davi...\")\n",
    "\n",
    "# Carregar dados específicos do Davi\n",
    "davi_windows, davi_labels, _ = load_openbci_eeg(Path(DAVI_DATA_PATH))\n",
    "print(f\"📊 Dados carregados: {davi_windows.shape[0]} amostras\")\n",
    "\n",
    "# Parâmetros do fine-tuning\n",
    "fine_tuning_params = {\n",
    "    'freeze_strategy': 'early',      # congela camadas iniciais\n",
    "    'learning_rate_ratio': 0.1,      # lr menor para fine-tuning\n",
    "    'epochs': 30,\n",
    "    'batch_size': 8,\n",
    "    'early_stopping_patience': 5,\n",
    "    'learning_rate': TRAINING_PARAMS['learning_rate'],\n",
    "    'use_original_normalization': False  # Escolha se quer usar normalização original ou nova\n",
    "}\n",
    "\n",
    "# Executar fine-tuning\n",
    "if main_results and main_results.get('best_model_state') is not None:\n",
    "    print(\"\\n🎯 Iniciando fine-tuning com modelo base...\")\n",
    "\n",
    "    # Criar modelo base com os pesos do melhor modelo\n",
    "    base_model = EEGInceptionERPModel(\n",
    "        n_chans=davi_windows.shape[1],\n",
    "        n_outputs=len(np.unique(davi_labels)),\n",
    "        n_times=davi_windows.shape[2]\n",
    "    )\n",
    "    base_model.load_state_dict(main_results['best_model_state'])\n",
    "\n",
    "    # Fine-tuning\n",
    "    ft_results = fine_tune_model(\n",
    "        base_model=base_model,\n",
    "        windows=davi_windows,\n",
    "        labels=davi_labels,\n",
    "        params=fine_tuning_params,\n",
    "        original_norm_stats=main_results.get('normalization_stats')\n",
    "    )\n",
    "\n",
    "    # Salvar modelo final\n",
    "    if ft_results['model_state'] is not None:\n",
    "        base_model.load_state_dict(ft_results['model_state'])\n",
    "        model_path = MODELS_PATH / f\"{TRAINING_PARAMS['model_name']}_finetuned.pt\"\n",
    "        torch.save({\n",
    "            'model_state_dict': ft_results['model_state'],\n",
    "            'test_accuracy': ft_results['final_test_accuracy'],\n",
    "            'val_accuracy': ft_results['best_val_accuracy'],\n",
    "            'test_loss': ft_results['test_loss'],\n",
    "            'history': ft_results['history'],\n",
    "            'normalization_validation': ft_results['normalization_validation']\n",
    "        }, model_path)\n",
    "        print(f\"\\n💾 Modelo fine-tuned salvo em: {model_path}\")\n",
    "else:\n",
    "    print(\"❌ Modelo base não disponível para fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e7486",
   "metadata": {
    "id": "9b7e7486"
   },
   "source": [
    "## 8. Relatório Final\n",
    "\n",
    "Resumo da execução no Google Colab com implementação inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac7f43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0ac7f43",
    "outputId": "f431250f-c9ff-4cfa-feb4-e04f30dbb3f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "            RELATÓRIO FINAL - GOOGLE COLAB\n",
      "============================================================\n",
      "\n",
      "🖥️ AMBIENTE:\n",
      "  - Plataforma: Google Colab\n",
      "  - Braindecode: ✅ Disponível\n",
      "  - Device: cpu\n",
      "\n",
      "📊 DADOS:\n",
      "  - Tipo: Sintético\n",
      "  - Amostras: 3530\n",
      "  - Subjects: 79\n",
      "  - Classes: 2\n",
      "\n",
      "❌ RESULTADOS: FALHOU\n",
      "\n",
      "📁 ARQUIVOS CRIADOS:\n",
      "  - models/: 6 arquivo(s)\n",
      "  - results/: 2 arquivo(s)\n",
      "\n",
      "🎉 CONCLUSÃO:\n",
      "  Este notebook demonstrou como adaptar o pipeline BCI\n",
      "  para execução no Google Colab, incluindo:\n",
      "  • Implementação inline das classes principais\n",
      "  • Geração de dados sintéticos para teste\n",
      "  • Pipeline de treinamento simplificado\n",
      "  • Fallback para quando braindecode não está disponível\n",
      "\n",
      "✨ Execução concluída em: 2025-06-16 03:23:59\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"            RELATÓRIO FINAL - GOOGLE COLAB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n🖥️ AMBIENTE:\")\n",
    "print(f\"  - Plataforma: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
    "print(f\"  - Braindecode: {'✅ Disponível' if BRAINDECODE_AVAILABLE else '❌ Fallback CNN'}\")\n",
    "print(f\"  - Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n",
    "\n",
    "print(f\"\\n📊 DADOS:\")\n",
    "print(f\"  - Tipo: Sintético\")\n",
    "print(f\"  - Amostras: {len(labels)}\")\n",
    "print(f\"  - Subjects: {len(np.unique(subject_ids))}\")\n",
    "print(f\"  - Classes: {len(np.unique(labels))}\")\n",
    "\n",
    "if main_results:\n",
    "    print(f\"\\n🎯 RESULTADOS:\")\n",
    "    print(f\"  - Modelo: {main_results['model_name']}\")\n",
    "    print(f\"  - CV Accuracy: {main_results['cv_mean_accuracy']:.4f} ± {main_results['cv_std_accuracy']:.4f}\")\n",
    "    print(f\"  - Test Accuracy: {main_results['final_test_accuracy']:.4f}\")\n",
    "    print(f\"  - Status: ✅ SUCESSO\")\n",
    "else:\n",
    "    print(f\"\\n❌ RESULTADOS: FALHOU\")\n",
    "\n",
    "print(f\"\\n📁 ARQUIVOS CRIADOS:\")\n",
    "for path in [MODELS_PATH, RESULTS_PATH]:\n",
    "    if path.exists():\n",
    "        files = list(path.rglob(\"*\"))\n",
    "        print(f\"  - {path.name}/: {len(files)} arquivo(s)\")\n",
    "\n",
    "print(f\"\\n🎉 CONCLUSÃO:\")\n",
    "print(f\"  Este notebook demonstrou como adaptar o pipeline BCI\")\n",
    "print(f\"  para execução no Google Colab, incluindo:\")\n",
    "print(f\"  • Implementação inline das classes principais\")\n",
    "print(f\"  • Geração de dados sintéticos para teste\")\n",
    "print(f\"  • Pipeline de treinamento simplificado\")\n",
    "print(f\"  • Fallback para quando braindecode não está disponível\")\n",
    "\n",
    "print(f\"\\n✨ Execução concluída em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
