{"cells": [{"cell_type": "markdown", "id": "725a5162", "metadata": {}, "source": ["# BCI Training & Fine-Tuning Pipeline Test - Google Colab Version\n", "\n", "Este notebook testa o pipeline completo no Google Colab:\n", "1. **Setup**: Instalar depend\u00eancias e clonar reposit\u00f3rio\n", "2. **Treinamento Principal**: Subjects 1-79 do dataset PhysioNet\n", "3. **Fine-Tuning**: Dados espec\u00edficos da pasta Davi\n", "4. **Valida\u00e7\u00e3o**: M\u00e9tricas e compara\u00e7\u00e3o de performance\n", "\n", "## \u26a0\ufe0f IMPORTANTE: Este notebook foi adaptado para Google Colab"]}, {"cell_type": "markdown", "id": "021874a4", "metadata": {}, "source": ["## 1. Setup Inicial no Google Colab\n", "\n", "Vamos instalar as depend\u00eancias e configurar o ambiente adequadamente."]}, {"cell_type": "code", "execution_count": null, "id": "ecdd2a3e", "metadata": {}, "outputs": [], "source": ["# === INSTALA\u00c7\u00c3O DE DEPEND\u00caNCIAS ===\n", "print(\"\ud83d\udd27 Instalando depend\u00eancias necess\u00e1rias...\")\n", "\n", "# Instalar braindecode e depend\u00eancias cient\u00edficas\n", "!pip install braindecode mne torch torchvision torchaudio\n", "!pip install scikit-learn pandas numpy matplotlib seaborn\n", "!pip install scipy jupyter\n", "\n", "print(\"\u2705 Depend\u00eancias instaladas!\")\n", "\n", "# Verificar se est\u00e1 no Colab\n", "try:\n", "    import google.colab\n", "    IN_COLAB = True\n", "    print(\"\ud83d\udcf1 Executando no Google Colab\")\n", "except ImportError:\n", "    IN_COLAB = False\n", "    print(\"\ud83d\udcbb Executando localmente\")"]}, {"cell_type": "code", "execution_count": null, "id": "cf49bd97", "metadata": {}, "outputs": [], "source": ["# === CLONE DO REPOSIT\u00d3RIO (OPCIONAL) ===\n", "import os\n", "from pathlib import Path\n", "\n", "if IN_COLAB:\n", "    print(\"\ud83d\udd04 Configurando para Google Colab...\")\n", "    \n", "    # Se voc\u00ea quiser clonar seu reposit\u00f3rio:\n", "    # !git clone https://github.com/seu-usuario/projetoBCI.git\n", "    # os.chdir('/content/projetoBCI')\n", "    \n", "    # Por enquanto, vamos trabalhar no diret\u00f3rio padr\u00e3o do Colab\n", "    project_root = Path('/content')\n", "    \n", "    # Criar estrutura de diret\u00f3rios\n", "    (Path(\"/content/drive/MyDrive/Colab Notebooks/eeg_data\")).mkdir(exist_ok=True)\n", "    (project_root / \"models\").mkdir(exist_ok=True)\n", "    (project_root / \"results\").mkdir(exist_ok=True)\n", "    \n", "else:\n", "    # Executando localmente\n", "    project_root = Path(os.getcwd()).parent\n", "\n", "print(f\"\ud83d\udcc2 Diret\u00f3rio de trabalho: {project_root}\")\n", "print(f\"\ud83d\udcc1 Estrutura criada!\")"]}, {"cell_type": "markdown", "id": "244c91b8", "metadata": {}, "source": ["## 2. Implementa\u00e7\u00e3o das Classes Base (Inline)\n", "\n", "J\u00e1 que n\u00e3o temos o reposit\u00f3rio clonado, vamos implementar as classes principais inline."]}, {"cell_type": "code", "execution_count": null, "id": "3e4587fa", "metadata": {}, "outputs": [], "source": ["# === IMPLEMENTA\u00c7\u00c3O INLINE DAS CLASSES BASE ===\n", "import torch\n", "import torch.nn as nn\n", "import numpy as np\n", "from scipy import signal\n", "from typing import Optional, List, Tuple, Dict, Union\n", "import pandas as pd\n", "from datetime import datetime\n", "import json\n", "import logging\n", "from sklearn.model_selection import train_test_split, KFold\n", "from sklearn.preprocessing import StandardScaler\n", "from torch.utils.data import Dataset, DataLoader\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from pathlib import Path\n", "\n", "# Configurar logging\n", "logging.basicConfig(level=logging.INFO)\n", "logger = logging.getLogger(__name__)\n", "\n", "print(\"\u2705 Imports b\u00e1sicos carregados!\")"]}, {"cell_type": "code", "execution_count": null, "id": "1a7fe101", "metadata": {}, "outputs": [], "source": ["# === UNIVERSAL EEG NORMALIZER ===\n", "class UniversalEEGNormalizer:\n", "    \"\"\"Normalizador universal para dados EEG\"\"\"\n", "    \n", "    def __init__(self, method: str = 'zscore', mode: str = 'training'):\n", "        self.method = method\n", "        self.mode = mode\n", "        self.global_stats = {}\n", "        self.is_fitted = False\n", "    \n", "    def _ensure_3d(self, data: np.ndarray) -> np.ndarray:\n", "        \"\"\"Garantir que dados estejam em 3D (n_samples, n_channels, n_timepoints)\"\"\"\n", "        if len(data.shape) == 2:\n", "            n_samples, n_features = data.shape\n", "            if n_features % 16 == 0:  # Assumir 16 canais\n", "                n_channels = 16\n", "                n_timepoints = n_features // n_channels\n", "                data = data.reshape(n_samples, n_channels, n_timepoints)\n", "            else:\n", "                data = data[:, np.newaxis, :]\n", "        elif len(data.shape) == 1:\n", "            data = data[np.newaxis, np.newaxis, :]\n", "        return data\n", "    \n", "    def fit(self, data: np.ndarray):\n", "        \"\"\"Ajustar normalizador aos dados\"\"\"\n", "        data_3d = self._ensure_3d(data)\n", "        \n", "        if self.method == 'zscore':\n", "            self.global_stats['mean'] = np.mean(data_3d, axis=(0, 2), keepdims=True)\n", "            self.global_stats['std'] = np.std(data_3d, axis=(0, 2), keepdims=True)\n", "            self.global_stats['std'] = np.where(self.global_stats['std'] == 0, 1.0, self.global_stats['std'])\n", "        \n", "        self.is_fitted = True\n", "        return self\n", "    \n", "    def transform(self, data: np.ndarray) -> np.ndarray:\n", "        \"\"\"Transformar dados\"\"\"\n", "        if not self.is_fitted:\n", "            raise ValueError(\"Normalizador deve ser ajustado antes da transforma\u00e7\u00e3o\")\n", "        \n", "        original_shape = data.shape\n", "        data_3d = self._ensure_3d(data)\n", "        \n", "        if self.method == 'zscore':\n", "            normalized = (data_3d - self.global_stats['mean']) / self.global_stats['std']\n", "        \n", "        # Restaurar forma original\n", "        if len(original_shape) == 2:\n", "            return normalized.reshape(original_shape[0], -1)\n", "        elif len(original_shape) == 1:\n", "            return normalized.flatten()\n", "        return normalized\n", "    \n", "    def fit_transform(self, data: np.ndarray) -> np.ndarray:\n", "        \"\"\"Ajustar e transformar em um passo\"\"\"\n", "        return self.fit(data).transform(data)\n", "\n", "print(\"\u2705 UniversalEEGNormalizer implementado!\")"]}, {"cell_type": "code", "execution_count": null, "id": "12e6ebcc", "metadata": {}, "outputs": [], "source": ["# === BCI DATASET CLASS ===\n", "class BCIDataset(Dataset):\n", "    \"\"\"Dataset PyTorch para dados EEG\"\"\"\n", "    \n", "    def __init__(self, windows: np.ndarray, labels: np.ndarray, transform=None, augment: bool = False):\n", "        self.windows = torch.from_numpy(windows).float()\n", "        self.labels = torch.from_numpy(labels).long()\n", "        self.transform = transform\n", "        self.augment = augment\n", "    \n", "    def __len__(self):\n", "        return len(self.windows)\n", "    \n", "    def __getitem__(self, idx):\n", "        window = self.windows[idx]\n", "        label = self.labels[idx]\n", "        \n", "        if self.augment:\n", "            # Adicionar ru\u00eddo leve\n", "            if torch.rand(1) < 0.3:\n", "                noise = torch.randn_like(window) * 0.01\n", "                window = window + noise\n", "        \n", "        if self.transform:\n", "            window = self.transform(window)\n", "        \n", "        return window, label\n", "\n", "print(\"\u2705 BCIDataset implementado!\")"]}, {"cell_type": "code", "execution_count": null, "id": "bec4ebbc", "metadata": {}, "outputs": [], "source": ["# === EEG INCEPTION ERP MODEL ===\n", "try:\n", "    from braindecode.models import EEGInceptionERP\n", "    print(\"\u2705 Braindecode EEGInceptionERP importado com sucesso!\")\n", "    BRAINDECODE_AVAILABLE = True\n", "except ImportError as e:\n", "    print(f\"\u274c Erro ao importar braindecode: {e}\")\n", "    print(\"\ud83d\udd27 Vamos implementar um modelo CNN simples como fallback\")\n", "    BRAINDECODE_AVAILABLE = False\n", "\n", "class EEGInceptionERPModel(nn.Module):\n", "    \"\"\"Wrapper para EEGInceptionERP com fallback para CNN simples\"\"\"\n", "    \n", "    def __init__(self, n_chans: int, n_outputs: int, n_times: int, sfreq: float = 125.0, **kwargs):\n", "        super().__init__()\n", "        self.n_chans = n_chans\n", "        self.n_outputs = n_outputs\n", "        self.n_times = n_times\n", "        self.sfreq = sfreq\n", "        self.is_trained = False\n", "        \n", "        if BRAINDECODE_AVAILABLE:\n", "            try:\n", "                self.model = EEGInceptionERP(\n", "                    n_chans=n_chans,\n", "                    n_outputs=n_outputs,\n", "                    n_times=n_times,\n", "                    sfreq=sfreq\n", "                )\n", "                self.model_type = \"EEGInceptionERP\"\n", "                print(f\"\u2705 Usando EEGInceptionERP: {n_chans} canais, {n_times} pontos temporais\")\n", "            except Exception as e:\n", "                print(f\"\u26a0\ufe0f Erro ao criar EEGInceptionERP: {e}\")\n", "                self.model = self._create_fallback_cnn(n_chans, n_outputs, n_times)\n", "                self.model_type = \"FallbackCNN\"\n", "        else:\n", "            self.model = self._create_fallback_cnn(n_chans, n_outputs, n_times)\n", "            self.model_type = \"FallbackCNN\"\n", "    \n", "    def _create_fallback_cnn(self, n_chans: int, n_outputs: int, n_times: int):\n", "        \"\"\"Criar CNN simples como fallback\"\"\"\n", "        print(f\"\ud83d\udd27 Criando CNN fallback: {n_chans} canais, {n_times} pontos temporais\")\n", "        return nn.Sequential(\n", "            # Convolu\u00e7\u00e3o temporal\n", "            nn.Conv1d(n_chans, 32, kernel_size=25, padding=12),\n", "            nn.BatchNorm1d(32),\n", "            nn.ReLU(),\n", "            nn.Dropout(0.25),\n", "            \n", "            # Redu\u00e7\u00e3o de dimensionalidade\n", "            nn.Conv1d(32, 64, kernel_size=15, padding=7),\n", "            nn.BatchNorm1d(64),\n", "            nn.ReLU(),\n", "            nn.MaxPool1d(4),\n", "            nn.Dropout(0.25),\n", "            \n", "            # Convolu\u00e7\u00e3o final\n", "            nn.Conv1d(64, 128, kernel_size=7, padding=3),\n", "            nn.BatchNorm1d(128),\n", "            nn.ReLU(),\n", "            nn.AdaptiveAvgPool1d(1),\n", "            nn.Flatten(),\n", "            \n", "            # Classificador\n", "            nn.Linear(128, 64),\n", "            nn.ReLU(),\n", "            nn.Dropout(0.5),\n", "            nn.Linear(64, n_outputs)\n", "        )\n", "    \n", "    def forward(self, x):\n", "        if self.model_type == \"EEGInceptionERP\":\n", "            return self.model(x)\n", "        else:\n", "            # Para o CNN fallback, precisamos transpor de (batch, channels, time) para (batch, channels, time)\n", "            return self.model(x)\n", "    \n", "    def save(self, filepath: str):\n", "        \"\"\"Salvar modelo\"\"\"\n", "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n", "        save_dict = {\n", "            'model_state_dict': self.model.state_dict(),\n", "            'model_type': self.model_type,\n", "            'n_chans': self.n_chans,\n", "            'n_outputs': self.n_outputs,\n", "            'n_times': self.n_times,\n", "            'sfreq': self.sfreq,\n", "            'is_trained': self.is_trained\n", "        }\n", "        torch.save(save_dict, filepath)\n", "        print(f\"\u2705 Modelo salvo: {filepath}\")\n", "    \n", "    def load(self, filepath: str):\n", "        \"\"\"Carregar modelo\"\"\"\n", "        checkpoint = torch.load(filepath, map_location='cpu')\n", "        self.model.load_state_dict(checkpoint['model_state_dict'])\n", "        self.is_trained = checkpoint.get('is_trained', True)\n", "        print(f\"\u2705 Modelo carregado: {filepath}\")\n", "\n", "print(f\"\u2705 EEGInceptionERPModel implementado! Tipo: {'Braindecode' if BRAINDECODE_AVAILABLE else 'Fallback CNN'}\")"]}, {"cell_type": "markdown", "id": "3099f820", "metadata": {}, "source": ["## 3. Configura\u00e7\u00e3o dos Caminhos e Par\u00e2metros"]}, {"cell_type": "code", "execution_count": null, "id": "b8071299", "metadata": {}, "outputs": [], "source": ["# Configura\u00e7\u00e3o dos caminhos para Colab\n", "EEG_DATA_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/eeg_data\")\n", "DAVI_DATA_PATH = EEG_DATA_PATH / \"Davi\"\n", "MODELS_PATH = project_root / \"models\"\n", "RESULTS_PATH = project_root / \"results\"\n", "\n", "# Par\u00e2metros de treinamento (ajustados para teste r\u00e1pido no Colab)\n", "MAIN_TRAINING_PARAMS = {\n", "    \"subjects_to_use\": [1, 2, 3],  # TESTE: apenas 3 subjects\n", "    \"num_epochs_per_fold\": 5,  # TESTE: apenas 5 epochs\n", "    \"num_k_folds\": 3,  # TESTE: apenas 3 folds\n", "    \"learning_rate\": 0.001,\n", "    \"early_stopping_patience\": 3,\n", "    \"batch_size\": 8,  # TESTE: batch menor\n", "    \"test_split_ratio\": 0.2,\n", "    \"data_base_path\": str(EEG_DATA_PATH),\n", "    \"model_name\": \"colab_test_model\"\n", "}\n", "\n", "# Par\u00e2metros de fine-tuning\n", "FINE_TUNING_PARAMS = {\n", "    \"freeze_strategy\": \"early\",\n", "    \"learning_rate_ratio\": 0.1,\n", "    \"epochs\": 10,  # TESTE: poucos epochs\n", "    \"batch_size\": 8,\n", "    \"validation_split\": 0.2\n", "}\n", "\n", "print(\"\u2705 Configura\u00e7\u00f5es definidas para Google Colab!\")\n", "print(f\"\ud83d\udcc2 Diret\u00f3rio de dados: {EEG_DATA_PATH}\")\n", "print(f\"\ud83c\udfd7\ufe0f Modelos: {MODELS_PATH}\")\n", "print(f\"\ud83d\udcca Resultados: {RESULTS_PATH}\")\n", "print(\"\\n\u26a0\ufe0f NOTA: Par\u00e2metros reduzidos para teste r\u00e1pido no Colab\")"]}, {"cell_type": "markdown", "id": "4779d717", "metadata": {}, "source": ["## 4. Gera\u00e7\u00e3o de Dados Sint\u00e9ticos para Teste\n", "\n", "J\u00e1 que provavelmente voc\u00ea n\u00e3o tem os dados reais no Colab, vamos gerar dados sint\u00e9ticos para testar o pipeline."]}, {"cell_type": "code", "execution_count": null, "id": "b5279943", "metadata": {}, "outputs": [], "source": ["# === GERA\u00c7\u00c3O DE DADOS SINT\u00c9TICOS ===\n", "def generate_synthetic_/content/drive/MyDrive/Colab Notebooks/eeg_data(n_subjects: int = 3, n_sessions_per_subject: int = 3):\n", "    \"\"\"Gerar dados EEG sint\u00e9ticos para teste\"\"\"\n", "    print(f\"\ud83c\udfb2 Gerando dados sint\u00e9ticos: {n_subjects} subjects, {n_sessions_per_subject} sess\u00f5es cada\")\n", "    \n", "    # Par\u00e2metros dos dados\n", "    n_channels = 16\n", "    n_timepoints = 400  # 3.2s a 125Hz\n", "    n_trials_per_session = 50\n", "    \n", "    all_windows = []\n", "    all_labels = []\n", "    all_subject_ids = []\n", "    \n", "    for subject_id in range(1, n_subjects + 1):\n", "        for session in range(n_sessions_per_subject):\n", "            # Gerar dados com padr\u00f5es diferentes por classe\n", "            for trial in range(n_trials_per_session):\n", "                # Classe aleat\u00f3ria (0: m\u00e3o esquerda, 1: m\u00e3o direita)\n", "                label = np.random.randint(0, 2)\n", "                \n", "                # Gerar sinal base\n", "                window = np.random.randn(n_channels, n_timepoints) * 0.1\n", "                \n", "                # Adicionar padr\u00e3o espec\u00edfico da classe\n", "                if label == 0:  # M\u00e3o esquerda - mais atividade em C3 (canal 7)\n", "                    window[7] += np.sin(np.linspace(0, 4*np.pi, n_timepoints)) * 0.3\n", "                    window[8] += np.cos(np.linspace(0, 3*np.pi, n_timepoints)) * 0.2\n", "                else:  # M\u00e3o direita - mais atividade em C4 (canal 9)\n", "                    window[9] += np.sin(np.linspace(0, 4*np.pi, n_timepoints)) * 0.3\n", "                    window[10] += np.cos(np.linspace(0, 3*np.pi, n_timepoints)) * 0.2\n", "                \n", "                # Adicionar ru\u00eddo espec\u00edfico do subject\n", "                subject_noise = np.random.randn(n_channels, n_timepoints) * (0.05 + subject_id * 0.01)\n", "                window += subject_noise\n", "                \n", "                all_windows.append(window)\n", "                all_labels.append(label)\n", "                all_subject_ids.append(subject_id)\n", "    \n", "    return np.array(all_windows), np.array(all_labels), np.array(all_subject_ids)\n", "\n", "# Gerar dados sint\u00e9ticos\n", "print(\"\ud83c\udfb2 Gerando dataset sint\u00e9tico...\")\n", "synthetic_windows, synthetic_labels, synthetic_subject_ids = generate_synthetic_/content/drive/MyDrive/Colab Notebooks/eeg_data()\n", "\n", "print(f\"\u2705 Dataset sint\u00e9tico criado:\")\n", "print(f\"   - Formato: {synthetic_windows.shape}\")\n", "print(f\"   - Labels: {np.bincount(synthetic_labels)}\")\n", "print(f\"   - Subjects: {np.unique(synthetic_subject_ids)}\")\n", "\n", "# Salvar dados sint\u00e9ticos\n", "synthetic_data_path = EEG_DATA_PATH / \"synthetic_data.npz\"\n", "EEG_DATA_PATH.mkdir(exist_ok=True)\n", "np.savez(synthetic_data_path, \n", "         windows=synthetic_windows, \n", "         labels=synthetic_labels, \n", "         subject_ids=synthetic_subject_ids)\n", "print(f\"\ud83d\udcbe Dados sint\u00e9ticos salvos em: {synthetic_data_path}\")"]}, {"cell_type": "markdown", "id": "c837787c", "metadata": {}, "source": ["## 5. Implementa\u00e7\u00e3o do Pipeline de Treinamento Simplificado"]}, {"cell_type": "code", "execution_count": null, "id": "304b93b1", "metadata": {}, "outputs": [], "source": ["# === PIPELINE DE TREINAMENTO SIMPLIFICADO ===\n", "def train_simplified_model(windows, labels, subject_ids, params):\n", "    \"\"\"Pipeline de treinamento simplificado para Colab\"\"\"\n", "    \n", "    print(\"\ud83d\ude80 Iniciando treinamento simplificado...\")\n", "    \n", "    # Filtrar subjects\n", "    if isinstance(params[\"subjects_to_use\"], list):\n", "        mask = np.isin(subject_ids, params[\"subjects_to_use\"])\n", "        windows = windows[mask]\n", "        labels = labels[mask]\n", "        subject_ids = subject_ids[mask]\n", "    \n", "    print(f\"\ud83d\udcca Dados filtrados: {windows.shape[0]} amostras\")\n", "    \n", "    # Normalizar dados\n", "    normalizer = UniversalEEGNormalizer(method='zscore')\n", "    windows_norm = normalizer.fit_transform(windows)\n", "    \n", "    # Split test\n", "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n", "        windows_norm, labels, \n", "        test_size=params[\"test_split_ratio\"], \n", "        random_state=42, \n", "        stratify=labels\n", "    )\n", "    \n", "    # Device\n", "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n", "    print(f\"\ud83d\udda5\ufe0f Usando device: {device}\")\n", "    \n", "    # K-fold CV\n", "    kfold = KFold(n_splits=params[\"num_k_folds\"], shuffle=True, random_state=42)\n", "    fold_results = []\n", "    \n", "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train_val), 1):\n", "        print(f\"\\n\ud83d\udcc1 Treinando fold {fold}/{params['num_k_folds']}...\")\n", "        \n", "        # Split fold\n", "        X_train_fold = X_train_val[train_idx]\n", "        X_val_fold = X_train_val[val_idx]\n", "        y_train_fold = y_train_val[train_idx]\n", "        y_val_fold = y_train_val[val_idx]\n", "        \n", "        # Datasets\n", "        train_dataset = BCIDataset(X_train_fold, y_train_fold, augment=True)\n", "        val_dataset = BCIDataset(X_val_fold, y_val_fold, augment=False)\n", "        \n", "        train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n", "        val_loader = DataLoader(val_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n", "        \n", "        # Modelo\n", "        model = EEGInceptionERPModel(\n", "            n_chans=windows.shape[1],\n", "            n_outputs=len(np.unique(labels)),\n", "            n_times=windows.shape[2]\n", "        ).to(device)\n", "        \n", "        # Treinamento\n", "        criterion = nn.CrossEntropyLoss()\n", "        optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n", "        \n", "        best_val_acc = 0.0\n", "        patience_counter = 0\n", "        \n", "        for epoch in range(params[\"num_epochs_per_fold\"]):\n", "            # Training\n", "            model.train()\n", "            train_loss = 0.0\n", "            train_correct = 0\n", "            train_total = 0\n", "            \n", "            for batch_x, batch_y in train_loader:\n", "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n", "                \n", "                optimizer.zero_grad()\n", "                outputs = model(batch_x)\n", "                loss = criterion(outputs, batch_y)\n", "                loss.backward()\n", "                optimizer.step()\n", "                \n", "                train_loss += loss.item()\n", "                _, predicted = torch.max(outputs.data, 1)\n", "                train_total += batch_y.size(0)\n", "                train_correct += (predicted == batch_y).sum().item()\n", "            \n", "            # Validation\n", "            model.eval()\n", "            val_loss = 0.0\n", "            val_correct = 0\n", "            val_total = 0\n", "            \n", "            with torch.no_grad():\n", "                for batch_x, batch_y in val_loader:\n", "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n", "                    outputs = model(batch_x)\n", "                    loss = criterion(outputs, batch_y)\n", "                    \n", "                    val_loss += loss.item()\n", "                    _, predicted = torch.max(outputs.data, 1)\n", "                    val_total += batch_y.size(0)\n", "                    val_correct += (predicted == batch_y).sum().item()\n", "            \n", "            train_acc = train_correct / train_total\n", "            val_acc = val_correct / val_total\n", "            \n", "            print(f\"  Epoch {epoch+1}: Train Acc: {train_acc:.3f}, Val Acc: {val_acc:.3f}\")\n", "            \n", "            # Early stopping\n", "            if val_acc > best_val_acc:\n", "                best_val_acc = val_acc\n", "                patience_counter = 0\n", "                # Salvar melhor modelo do fold\n", "                model_path = MODELS_PATH / params[\"model_name\"] / f\"fold_{fold}.pth\"\n", "                model_path.parent.mkdir(parents=True, exist_ok=True)\n", "                model.save(str(model_path))\n", "            else:\n", "                patience_counter += 1\n", "                if patience_counter >= params[\"early_stopping_patience\"]:\n", "                    print(f\"  Early stopping em epoch {epoch+1}\")\n", "                    break\n", "        \n", "        fold_results.append(best_val_acc)\n", "        print(f\"\u2705 Fold {fold} conclu\u00eddo - Melhor Val Acc: {best_val_acc:.4f}\")\n", "    \n", "    # Treinamento final\n", "    print(f\"\\n\ud83c\udfaf Treinamento final em todos os dados...\")\n", "    final_dataset = BCIDataset(X_train_val, y_train_val, augment=True)\n", "    test_dataset = BCIDataset(X_test, y_test, augment=False)\n", "    \n", "    final_train_loader = DataLoader(final_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n", "    test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False)\n", "    \n", "    final_model = EEGInceptionERPModel(\n", "        n_chans=windows.shape[1],\n", "        n_outputs=len(np.unique(labels)),\n", "        n_times=windows.shape[2]\n", "    ).to(device)\n", "    \n", "    criterion = nn.CrossEntropyLoss()\n", "    optimizer = torch.optim.Adam(final_model.parameters(), lr=params[\"learning_rate\"])\n", "    \n", "    # Treinar modelo final\n", "    for epoch in range(params[\"num_epochs_per_fold\"]):\n", "        final_model.train()\n", "        for batch_x, batch_y in final_train_loader:\n", "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n", "            optimizer.zero_grad()\n", "            outputs = final_model(batch_x)\n", "            loss = criterion(outputs, batch_y)\n", "            loss.backward()\n", "            optimizer.step()\n", "    \n", "    # Testar modelo final\n", "    final_model.eval()\n", "    test_correct = 0\n", "    test_total = 0\n", "    \n", "    with torch.no_grad():\n", "        for batch_x, batch_y in test_loader:\n", "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n", "            outputs = final_model(batch_x)\n", "            _, predicted = torch.max(outputs.data, 1)\n", "            test_total += batch_y.size(0)\n", "            test_correct += (predicted == batch_y).sum().item()\n", "    \n", "    final_test_acc = test_correct / test_total\n", "    \n", "    # Salvar modelo final\n", "    final_model_path = MODELS_PATH / params[\"model_name\"] / \"final_model.pth\"\n", "    final_model.save(str(final_model_path))\n", "    final_model.is_trained = True\n", "    \n", "    # Resultados\n", "    cv_mean = np.mean(fold_results)\n", "    cv_std = np.std(fold_results)\n", "    \n", "    results = {\n", "        'cv_mean_accuracy': cv_mean,\n", "        'cv_std_accuracy': cv_std,\n", "        'final_test_accuracy': final_test_acc,\n", "        'model_name': params[\"model_name\"],\n", "        'fold_accuracies': fold_results\n", "    }\n", "    \n", "    print(f\"\\n\ud83c\udf89 TREINAMENTO CONCLU\u00cdDO!\")\n", "    print(f\"   CV Mean: {cv_mean:.4f} \u00b1 {cv_std:.4f}\")\n", "    print(f\"   Test Acc: {final_test_acc:.4f}\")\n", "    \n", "    return results\n", "\n", "print(\"\u2705 Pipeline de treinamento simplificado implementado!\")"]}, {"cell_type": "markdown", "id": "41bc5b61", "metadata": {}, "source": ["## 6. Execu\u00e7\u00e3o do Treinamento Principal"]}, {"cell_type": "code", "execution_count": null, "id": "c1e3b1eb", "metadata": {}, "outputs": [], "source": ["# === EXECU\u00c7\u00c3O DO TREINAMENTO ===\n", "print(\"\ud83d\ude80 INICIANDO TREINAMENTO PRINCIPAL...\")\n", "\n", "try:\n", "    # Executar treinamento com dados sint\u00e9ticos\n", "    main_results = train_simplified_model(\n", "        synthetic_windows, \n", "        synthetic_labels, \n", "        synthetic_subject_ids, \n", "        MAIN_TRAINING_PARAMS\n", "    )\n", "    \n", "    print(\"\\n\ud83d\udcca RESULTADOS DO TREINAMENTO:\")\n", "    print(f\"  - CV Mean: {main_results['cv_mean_accuracy']:.4f} \u00b1 {main_results['cv_std_accuracy']:.4f}\")\n", "    print(f\"  - Test Acc: {main_results['final_test_accuracy']:.4f}\")\n", "    print(f\"  - Modelo: {main_results['model_name']}\")\n", "    \n", "    # Salvar resultados\n", "    results_path = RESULTS_PATH / \"main_training_results.json\"\n", "    RESULTS_PATH.mkdir(exist_ok=True)\n", "    with open(results_path, 'w') as f:\n", "        json.dump(main_results, f, indent=2, default=str)\n", "    \n", "    print(f\"\ud83d\udcbe Resultados salvos em: {results_path}\")\n", "    \n", "except Exception as e:\n", "    print(f\"\u274c Erro durante treinamento: {e}\")\n", "    import traceback\n", "    traceback.print_exc()\n", "    main_results = None"]}, {"cell_type": "markdown", "id": "9cd11f40", "metadata": {}, "source": ["## 7. Visualiza\u00e7\u00e3o dos Resultados"]}, {"cell_type": "code", "execution_count": null, "id": "ac95a0ce", "metadata": {}, "outputs": [], "source": ["# === VISUALIZA\u00c7\u00c3O DOS RESULTADOS ===\n", "if main_results:\n", "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n", "    fig.suptitle('Resultados do Treinamento BCI - Google Colab', fontsize=16, fontweight='bold')\n", "    \n", "    # 1. Acur\u00e1cias por fold\n", "    ax1 = axes[0, 0]\n", "    fold_accs = main_results['fold_accuracies']\n", "    ax1.bar(range(1, len(fold_accs) + 1), fold_accs, alpha=0.7, color='skyblue')\n", "    ax1.axhline(y=main_results['cv_mean_accuracy'], color='red', linestyle='--', \n", "                label=f\"Mean: {main_results['cv_mean_accuracy']:.3f}\")\n", "    ax1.set_xlabel('Fold')\n", "    ax1.set_ylabel('Acur\u00e1cia')\n", "    ax1.set_title('Acur\u00e1cia por Fold')\n", "    ax1.legend()\n", "    ax1.grid(True, alpha=0.3)\n", "    \n", "    # 2. Compara\u00e7\u00e3o CV vs Test\n", "    ax2 = axes[0, 1]\n", "    models = ['CV Mean', 'Test Final']\n", "    accuracies = [main_results['cv_mean_accuracy'], main_results['final_test_accuracy']]\n", "    bars = ax2.bar(models, accuracies, color=['lightcoral', 'lightgreen'], alpha=0.8)\n", "    ax2.set_ylabel('Acur\u00e1cia')\n", "    ax2.set_title('CV vs Test Performance')\n", "    ax2.set_ylim(0, 1)\n", "    \n", "    for bar, acc in zip(bars, accuracies):\n", "        height = bar.get_height()\n", "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n", "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n", "    \n", "    # 3. Distribui\u00e7\u00e3o das classes sint\u00e9ticas\n", "    ax3 = axes[1, 0]\n", "    unique, counts = np.unique(synthetic_labels, return_counts=True)\n", "    ax3.pie(counts, labels=[f'Classe {u}' for u in unique], autopct='%1.1f%%')\n", "    ax3.set_title('Distribui\u00e7\u00e3o de Classes')\n", "    \n", "    # 4. Resumo textual\n", "    ax4 = axes[1, 1]\n", "    ax4.axis('off')\n", "    summary_text = f\"\"\"\n", "    RESUMO DO TREINAMENTO\n", "    \n", "    Dataset: Sint\u00e9tico\n", "    Subjects: {len(np.unique(synthetic_subject_ids))}\n", "    Amostras: {len(synthetic_labels)}\n", "    \n", "    Modelo: {main_results['model_name']}\n", "    Tipo: {'EEGInceptionERP' if BRAINDECODE_AVAILABLE else 'CNN Fallback'}\n", "    \n", "    Performance:\n", "    \u2022 CV Mean: {main_results['cv_mean_accuracy']:.4f}\n", "    \u2022 CV Std: {main_results['cv_std_accuracy']:.4f}\n", "    \u2022 Test Acc: {main_results['final_test_accuracy']:.4f}\n", "    \n", "    Status: \u2705 Sucesso\n", "    \"\"\"\n", "    ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes, \n", "             fontsize=10, verticalalignment='top', fontfamily='monospace')\n", "    \n", "    plt.tight_layout()\n", "    plt.savefig(RESULTS_PATH / 'colab_training_results.png', dpi=150, bbox_inches='tight')\n", "    plt.show()\n", "    \n", "    print(\"\u2705 Visualiza\u00e7\u00e3o criada e salva!\")\n", "else:\n", "    print(\"\u274c N\u00e3o h\u00e1 resultados para visualizar\")"]}, {"cell_type": "markdown", "id": "9b7e7486", "metadata": {}, "source": ["## 8. Relat\u00f3rio Final\n", "\n", "Resumo da execu\u00e7\u00e3o no Google Colab com implementa\u00e7\u00e3o inline."]}, {"cell_type": "code", "execution_count": null, "id": "e0ac7f43", "metadata": {}, "outputs": [], "source": ["print(\"=\" * 60)\n", "print(\"            RELAT\u00d3RIO FINAL - GOOGLE COLAB\")\n", "print(\"=\" * 60)\n", "\n", "print(f\"\\n\ud83d\udda5\ufe0f AMBIENTE:\")\n", "print(f\"  - Plataforma: {'Google Colab' if IN_COLAB else 'Local'}\")\n", "print(f\"  - Braindecode: {'\u2705 Dispon\u00edvel' if BRAINDECODE_AVAILABLE else '\u274c Fallback CNN'}\")\n", "print(f\"  - Device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n", "\n", "print(f\"\\n\ud83d\udcca DADOS:\")\n", "print(f\"  - Tipo: Sint\u00e9tico\")\n", "print(f\"  - Amostras: {len(synthetic_labels)}\")\n", "print(f\"  - Subjects: {len(np.unique(synthetic_subject_ids))}\")\n", "print(f\"  - Classes: {len(np.unique(synthetic_labels))}\")\n", "\n", "if main_results:\n", "    print(f\"\\n\ud83c\udfaf RESULTADOS:\")\n", "    print(f\"  - Modelo: {main_results['model_name']}\")\n", "    print(f\"  - CV Accuracy: {main_results['cv_mean_accuracy']:.4f} \u00b1 {main_results['cv_std_accuracy']:.4f}\")\n", "    print(f\"  - Test Accuracy: {main_results['final_test_accuracy']:.4f}\")\n", "    print(f\"  - Status: \u2705 SUCESSO\")\n", "else:\n", "    print(f\"\\n\u274c RESULTADOS: FALHOU\")\n", "\n", "print(f\"\\n\ud83d\udcc1 ARQUIVOS CRIADOS:\")\n", "for path in [MODELS_PATH, RESULTS_PATH]:\n", "    if path.exists():\n", "        files = list(path.rglob(\"*\"))\n", "        print(f\"  - {path.name}/: {len(files)} arquivo(s)\")\n", "\n", "print(f\"\\n\ud83c\udf89 CONCLUS\u00c3O:\")\n", "print(f\"  Este notebook demonstrou como adaptar o pipeline BCI\")\n", "print(f\"  para execu\u00e7\u00e3o no Google Colab, incluindo:\")\n", "print(f\"  \u2022 Implementa\u00e7\u00e3o inline das classes principais\")\n", "print(f\"  \u2022 Gera\u00e7\u00e3o de dados sint\u00e9ticos para teste\")\n", "print(f\"  \u2022 Pipeline de treinamento simplificado\")\n", "print(f\"  \u2022 Fallback para quando braindecode n\u00e3o est\u00e1 dispon\u00edvel\")\n", "\n", "print(f\"\\n\u2728 Execu\u00e7\u00e3o conclu\u00edda em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n", "print(\"=\" * 60)"]}], "metadata": {"language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 5}